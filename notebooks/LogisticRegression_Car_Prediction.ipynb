{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leocorelli/ComputerVisionProject/blob/main/LogisticRegression_Car_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification of car models using non deep learning approach**.\\\n",
        "This notebook contains step by step guide on how to classify cars using logistic regression"
      ],
      "metadata": {
        "id": "RnElulp5W_U3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dependencies"
      ],
      "metadata": {
        "id": "GQX5Y6tmRtaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HW2qXibsFwJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google drive to access the data from the drive"
      ],
      "metadata": {
        "id": "Gjvvdx5XRzRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DodLnWgSs-eb",
        "outputId": "5ac3a00c-de98-49e6-8cad-f76c888b5709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point to the folder where data has to be downloaded"
      ],
      "metadata": {
        "id": "rcqY2uycR86t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmO6772GtH5S",
        "outputId": "95fe12f7-a3c9-4e9f-af56-e898a1a180e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/computerVision\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/computerVision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the training data"
      ],
      "metadata": {
        "id": "JLNqBt08SFvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjCZsqVsxLRC",
        "outputId": "fd49b36f-fc23-4a06-e6be-5ce78f9c97c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  train_real.zip\n",
            "replace train_real/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train_real/.DS_Store    \n",
            "replace __MACOSX/train_real/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/train_real/._.DS_Store  \n",
            "y\n"
          ]
        }
      ],
      "source": [
        "!unzip train_real.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the testing data"
      ],
      "metadata": {
        "id": "PLYKdiOTSNYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yn9gQgezlRI"
      },
      "outputs": [],
      "source": [
        "!unzip test_real.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to create custom pytorch datasets"
      ],
      "metadata": {
        "id": "_AwfsBXuSQYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G61uEqqsOYk"
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_image\n",
        "#from PIL import ImageFile\n",
        "#ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class CarsDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img_path = os.path.join(self.root_dir, self.annotations.iloc[index,0])\n",
        "    image = read_image(img_path)\n",
        "    label = int(self.annotations.iloc[index,-1])\n",
        "    \n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    \n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to apply different tranformations"
      ],
      "metadata": {
        "id": "N5-UQ3UfSqzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5g7YvS5sTsJ"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import ToPILImage\n",
        "from torch.utils import data\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([      \n",
        "        transforms.ToPILImage(),        \n",
        "        #transforms.RandomResizedCrop(224),\n",
        "        transforms.Resize(256),               # NOT IN ORIGINAL\n",
        "        transforms.CenterCrop(224),           # NOT ORIGINAL\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([    \n",
        "        transforms.ToPILImage(),                           \n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the training and test data from the custom method and augment the data by applying the transformations"
      ],
      "metadata": {
        "id": "kTZPW4Z1S9qs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryBCOHq6sYVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa814f43-d0dd-4ffc-c954-8d9a002e8740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train_dataset is: 16016\n",
            "Length of val_dataset is: 14939\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CarsDataset('train_w_converted.txt','./train_real', data_transforms['train'])\n",
        "val_dataset = CarsDataset('test_w_converted.txt','./test_real', data_transforms['val'])\n",
        "dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
        "\n",
        "print(f'Length of train_dataset is: {len(train_dataset)}')\n",
        "print(f'Length of val_dataset is: {len(val_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data loaders and split the data into batches"
      ],
      "metadata": {
        "id": "-NsKehTiTOjZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7suxwvStbwI"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=16)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=16)\n",
        "\n",
        "dataloaders = {'train':train_loader,'val':val_loader}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Now that we have prepared our data loaders, we can define our model.\n",
        "\n",
        "* A **logistic regression** model is almost identical to a linear regression model. It contains weights and bias matrices, and the output is obtained using simple matrix operations (`pred = x @ w.t() + b`). \n",
        "\n",
        "* As we did with linear regression, we can use `nn.Linear` to create the model instead of manually creating and initializing the matrices.\n",
        "\n",
        "* Since `nn.Linear` expects each training example to be a vector, each `3x224x224` image tensor is _flattened_ into a vector of size 150528 `(3*224*224)` before being passed into the model. \n",
        "\n",
        "* The output for each image is a vector of size 431, with each element signifying the probability of a particular target label. The predicted label for an image is simply the one with the highest probability."
      ],
      "metadata": {
        "id": "snDNnXKDTckC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI5J90x_05Lb"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 3*224*224\n",
        "num_classes = 431\n",
        "\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR0qezZs1CLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b824ef3e-b655-4e3b-a820-194bba42f083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([431, 150528])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 8.7692e-04, -1.5913e-03,  8.6228e-04,  ...,  8.6582e-05,\n",
              "          2.4944e-03,  2.4818e-03],\n",
              "        [ 4.5188e-04,  1.3916e-05, -7.4557e-04,  ..., -1.5437e-03,\n",
              "         -1.1841e-03,  1.7061e-03],\n",
              "        [-6.2313e-04,  1.9934e-03, -7.5949e-05,  ..., -1.1873e-03,\n",
              "          2.1115e-03,  1.0491e-03],\n",
              "        ...,\n",
              "        [ 7.8408e-04,  1.4902e-03, -1.0784e-03,  ..., -1.3958e-03,\n",
              "         -4.8483e-04, -1.4622e-03],\n",
              "        [-8.3008e-04, -1.7341e-03,  1.7390e-03,  ...,  1.0937e-03,\n",
              "          8.1005e-05,  5.8338e-04],\n",
              "        [-2.5181e-04, -1.5469e-03, -1.7024e-03,  ..., -1.9968e-03,\n",
              "          1.0928e-03,  2.0770e-03]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "print(model.weight.shape)\n",
        "model.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpWzH65m1V3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee6aac9-3334-491c-b5a2-fa35d9475899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([431])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-8.9636e-04, -4.0729e-04, -1.6041e-03,  2.2647e-03,  7.2139e-04,\n",
              "        -4.3837e-04,  1.2266e-03,  1.8798e-03, -6.3372e-06,  4.8769e-04,\n",
              "        -2.3492e-03, -2.5281e-03, -9.9846e-04, -4.9665e-05, -4.0633e-04,\n",
              "         1.3262e-03, -3.7169e-04,  1.8727e-03, -1.6118e-03,  1.2323e-03,\n",
              "        -8.3653e-04, -1.1722e-03,  1.1564e-03, -6.5791e-05,  2.3773e-03,\n",
              "         1.2527e-05,  1.0489e-03,  1.4988e-03, -1.8062e-04, -1.7605e-03,\n",
              "        -9.0189e-04,  1.7681e-03,  2.4089e-03,  1.1246e-03,  2.2029e-03,\n",
              "         1.0369e-03,  1.4137e-03,  1.0499e-03,  3.3570e-04, -3.8545e-04,\n",
              "         2.4597e-03, -3.5344e-04, -2.2720e-03, -7.8906e-04,  4.9216e-04,\n",
              "         1.2074e-03, -2.1755e-03,  1.0935e-04, -1.2365e-03, -7.0296e-04,\n",
              "         2.1764e-03, -6.5639e-04,  1.7697e-03, -1.1732e-04, -5.1099e-05,\n",
              "         1.0304e-03,  2.4803e-03,  1.4888e-04,  3.3759e-04, -6.0497e-04,\n",
              "        -6.6504e-04, -6.6975e-04, -1.5190e-03, -1.0370e-03,  1.5347e-03,\n",
              "        -6.6784e-04, -1.2191e-03, -1.5117e-03, -2.5528e-03, -2.2508e-03,\n",
              "        -2.4539e-03, -1.2831e-03, -1.2525e-03,  2.5656e-03,  1.2777e-03,\n",
              "         2.5538e-03,  2.1842e-03, -6.7667e-04,  2.1650e-03, -6.9706e-04,\n",
              "         3.3378e-04,  5.7623e-04,  5.3562e-04,  1.4943e-04,  2.2535e-03,\n",
              "         4.1235e-04, -1.8283e-03,  2.3193e-03, -6.9210e-04, -5.5414e-04,\n",
              "        -2.3712e-04,  1.7762e-03, -1.0551e-03,  1.1083e-03, -8.9099e-04,\n",
              "         8.4212e-04, -2.2396e-03,  1.8914e-03, -4.1182e-04, -1.1531e-03,\n",
              "         2.0740e-03,  1.5481e-03, -4.4840e-04, -1.6132e-03,  2.4376e-03,\n",
              "         1.2649e-03, -3.5498e-04,  3.9873e-04, -1.5202e-03,  2.4063e-03,\n",
              "        -4.0221e-04, -1.3134e-03,  1.9407e-03,  1.1495e-03, -1.4448e-03,\n",
              "         1.2545e-03,  1.8343e-03,  2.1830e-03,  1.2417e-03,  2.1575e-03,\n",
              "         1.4643e-03, -1.9681e-03, -2.3568e-03,  3.4375e-04,  1.1842e-03,\n",
              "        -6.4569e-04, -1.3313e-03, -1.7612e-03,  2.1592e-03,  1.1927e-03,\n",
              "         1.5511e-03,  2.8018e-04, -2.4695e-03, -2.4778e-03, -1.2300e-03,\n",
              "        -1.4349e-03, -2.0063e-04, -2.2128e-04, -1.7801e-03, -1.7504e-03,\n",
              "        -1.1591e-03,  2.7974e-04, -1.3750e-03,  2.4301e-03, -1.2156e-03,\n",
              "        -1.1486e-03, -1.3455e-03, -1.6180e-03,  1.6632e-03, -8.2643e-04,\n",
              "        -1.8723e-03, -2.2955e-03,  8.1086e-04, -1.7815e-03, -5.8066e-04,\n",
              "         1.9358e-03, -1.5101e-03,  5.1976e-04, -1.1421e-03, -1.7893e-03,\n",
              "        -4.5956e-04,  7.4921e-04,  1.2755e-03, -2.1780e-03, -1.2858e-04,\n",
              "        -2.1133e-03,  2.0250e-03,  7.6146e-04, -2.3303e-03,  1.1543e-03,\n",
              "        -2.0084e-03, -2.3152e-03, -6.6688e-04,  7.9610e-04, -4.3746e-04,\n",
              "        -1.6885e-03, -1.3739e-03, -2.2912e-03, -7.3921e-04,  1.0301e-04,\n",
              "         1.4729e-03, -9.9543e-04, -2.0173e-03,  7.7602e-04,  2.1430e-03,\n",
              "         1.1307e-03,  1.9992e-03,  2.2859e-03,  2.4086e-03,  8.9458e-04,\n",
              "        -4.4026e-04, -6.7110e-04,  2.4796e-03, -1.3224e-03, -3.0762e-04,\n",
              "        -1.0068e-04, -5.0871e-04, -1.1743e-03,  1.1335e-03,  1.7417e-03,\n",
              "        -6.5803e-04,  9.1889e-04,  2.0077e-03, -3.2376e-04,  1.6455e-03,\n",
              "        -8.4709e-04, -1.7255e-03, -1.9161e-03, -8.5055e-04, -1.0399e-03,\n",
              "         1.9850e-04, -1.1340e-04,  2.1365e-03,  2.5430e-03,  7.5675e-04,\n",
              "        -2.2995e-03, -2.1187e-03, -2.0301e-03, -6.3365e-04, -3.5339e-04,\n",
              "        -3.0572e-04,  2.1222e-03, -2.1241e-03, -2.3796e-03, -1.8009e-03,\n",
              "         9.3561e-04,  3.8332e-04, -8.4092e-04, -1.7716e-03, -7.7366e-04,\n",
              "        -2.3163e-03, -8.0392e-04,  7.9194e-04, -3.4351e-04,  1.4225e-03,\n",
              "        -1.7012e-03,  1.8710e-03,  1.7227e-03,  1.0092e-04, -1.0695e-03,\n",
              "        -1.7929e-03, -2.0003e-03, -2.4991e-03, -1.2292e-03, -7.6391e-04,\n",
              "         1.0807e-03,  5.1027e-04, -1.1163e-03, -2.1922e-03, -1.4717e-03,\n",
              "        -1.6949e-03,  1.5496e-03, -3.6699e-04, -2.3848e-03, -1.5769e-03,\n",
              "        -2.1215e-03, -1.0300e-04,  1.1013e-03,  1.7527e-03, -8.5035e-04,\n",
              "        -2.4002e-03, -1.9552e-03, -2.4902e-03,  5.5795e-04,  2.2784e-03,\n",
              "         6.6521e-05,  1.2198e-03, -1.3828e-03, -1.4285e-03,  3.1269e-04,\n",
              "         4.3694e-04,  2.1399e-03, -2.1164e-03, -1.2855e-03,  2.3228e-04,\n",
              "         2.4141e-03,  1.4179e-03,  1.0892e-03, -2.7134e-04,  2.5034e-03,\n",
              "        -1.4441e-03,  1.4185e-04,  1.7889e-03, -1.3106e-03, -2.0409e-03,\n",
              "         1.9076e-04,  2.0910e-03,  6.1854e-04, -7.3408e-04, -2.3052e-03,\n",
              "        -5.7159e-04,  8.1446e-04, -8.4418e-04,  1.8940e-03,  1.1918e-03,\n",
              "        -1.3795e-03,  1.1349e-03, -2.1468e-03, -9.3470e-04,  9.8712e-04,\n",
              "        -8.8106e-04, -2.3484e-03,  8.9762e-04, -2.3682e-03,  1.6332e-03,\n",
              "         1.4341e-03, -2.1664e-03, -2.3802e-04,  2.4809e-03, -2.2966e-03,\n",
              "         2.3509e-03, -1.3111e-03,  1.3107e-03,  1.5666e-03,  2.0281e-03,\n",
              "        -2.0399e-03,  3.6576e-04, -1.3664e-03,  1.6719e-04,  1.9852e-03,\n",
              "        -2.2774e-03,  1.4258e-03,  1.5161e-03, -2.4538e-03,  1.1049e-03,\n",
              "        -1.0175e-03, -9.8016e-04,  2.2279e-03,  1.3924e-03,  2.6655e-04,\n",
              "         1.4323e-03,  6.9721e-04, -1.8194e-03,  9.8097e-04, -8.0640e-04,\n",
              "         2.3231e-03,  1.7502e-03, -1.6693e-03, -3.0492e-04,  9.6411e-04,\n",
              "         7.5913e-04,  2.3721e-03,  2.1752e-03, -1.5693e-03,  2.4455e-03,\n",
              "        -2.4411e-03, -2.5385e-05, -2.2081e-04, -1.0334e-03, -2.0855e-03,\n",
              "        -4.5451e-04, -1.1069e-03, -7.3812e-04,  1.9500e-03,  1.5804e-03,\n",
              "        -2.4028e-03,  7.1370e-04, -1.4521e-03, -1.0839e-03,  6.5225e-04,\n",
              "        -1.7112e-03, -1.4264e-03,  1.3180e-03,  1.1965e-03, -2.1760e-03,\n",
              "        -6.3100e-04, -4.3931e-04,  1.4314e-04,  2.0370e-03, -1.1653e-03,\n",
              "        -1.2615e-04, -1.3971e-03,  5.4896e-04, -3.4061e-04,  5.6866e-04,\n",
              "         1.3493e-03, -1.4062e-04,  4.4954e-04,  1.2435e-03, -6.0703e-04,\n",
              "         4.0616e-04,  2.5556e-03,  2.0550e-03, -2.2683e-03,  1.7441e-03,\n",
              "         5.6192e-04,  5.3907e-04,  1.1458e-03,  3.2769e-04,  2.0460e-03,\n",
              "        -1.8506e-03,  1.9045e-03, -2.3315e-03, -1.5615e-03,  1.9119e-03,\n",
              "         7.6776e-04, -8.0793e-04,  2.4028e-03,  6.6831e-04,  1.3546e-03,\n",
              "        -4.3848e-04,  2.5678e-03, -1.9936e-03, -2.1467e-03,  2.4540e-03,\n",
              "        -1.9846e-04, -1.4875e-03, -2.6905e-04,  1.9209e-03, -9.0965e-04,\n",
              "         8.7583e-04,  2.0720e-04,  2.0399e-03, -2.1207e-03,  6.1346e-04,\n",
              "        -6.9224e-05, -8.9013e-05, -2.5697e-03,  6.1551e-04, -1.9359e-03,\n",
              "         1.2085e-03, -1.5368e-03, -2.3999e-03, -5.6007e-04, -9.4373e-04,\n",
              "         1.0992e-03, -2.2443e-03,  4.7938e-04, -1.5718e-03, -2.5229e-03,\n",
              "         9.4359e-04], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "print(model.bias.shape)\n",
        "model.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the image"
      ],
      "metadata": {
        "id": "8qBD0QutTtso"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gfe9AIE1ZU8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "646cfa4f-b18d-4f03-b095-687211cdafdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([370,  30,  30, 122, 429, 348, 210, 278,  85, 411, 299, 246, 388, 167,\n",
            "        175,  59, 208, 249, 333, 297, 210,  48,   9, 340, 314, 115, 360, 268,\n",
            "        148,  68, 145, 287])\n",
            "torch.Size([32, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d0fe7d306f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (21504x224 and 150528x431)"
          ]
        }
      ],
      "source": [
        "for images, labels in train_loader:\n",
        "    print(labels)\n",
        "    print(images.shape)\n",
        "    outputs = model(images)\n",
        "    print(outputs)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mEzf-Hh1kSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f43a6c2-ac07-4b8c-e1ef-43c899e1c2c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilJbYVab1ljf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96836469-ab12-4236-aabd-f4aa34e589fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 150528])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "images.reshape(32, 150528).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above leads to an error because our input data does not have the right shape. Our images are of the shape 3x28x28, but we need them to be vectors of size 150528, i.e., we need to flatten them. We'll use the `.reshape` method of a tensor, which will allow us to efficiently 'view' each image as a flat vector without really creating a copy of the underlying data. To include this additional functionality within our model, we need to define a custom model by extending the `nn.Module` class from PyTorch. "
      ],
      "metadata": {
        "id": "AqcnuIT1Uh0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classes can also build upon or _extend_ the functionality of existing classes. Let's extend the `nn.Module` class from PyTorch to define a custom model."
      ],
      "metadata": {
        "id": "XwIw1I7MUu41"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8FicyUe2fVq"
      },
      "outputs": [],
      "source": [
        "class CarModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 150528)\n",
        "        out = self.linear(xb)\n",
        "        return out\n",
        "    \n",
        "model = CarModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the `__init__` constructor method, we instantiate the weights and biases using `nn.Linear`. And inside the `forward` method, which is invoked when we pass a batch of inputs to the model, we flatten the input tensor and pass it into `self.linear`.\n",
        "\n",
        "`xb.reshape(-1, 150528)` indicates to PyTorch that we want a *view* of the `xb` tensor with two dimensions. The length along the 2nd dimension is 3\\*224\\*224 (i.e., 150528). One argument to `.reshape` can be set to `-1` (in this case, the first dimension) to let PyTorch figure it out automatically based on the shape of the original tensor.\n",
        "\n",
        "Note that the model no longer has `.weight` and `.bias` attributes (as they are now inside the `.linear` attribute), but it does have a `.parameters` method that returns a list containing the weights and bias."
      ],
      "metadata": {
        "id": "JGcesumTU3V9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MtK4nZs2mda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f775f7d-e535-465f-e0ba-188b9ebdeb22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=150528, out_features=431, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model.linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDrVWe_-2oNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d4165c-c46f-4ce5-ee1f-6168849fb3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([431, 150528]) torch.Size([431])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-1.1880e-03,  2.5362e-03,  1.2352e-03,  ...,  1.4591e-03,\n",
              "          -1.3256e-03,  5.2723e-04],\n",
              "         [ 2.1849e-03, -1.8072e-03,  2.3635e-03,  ..., -6.5830e-05,\n",
              "           3.6845e-04, -1.4525e-03],\n",
              "         [-3.8058e-04,  1.5349e-03,  1.5801e-03,  ..., -2.0406e-03,\n",
              "          -1.6218e-03,  1.0970e-03],\n",
              "         ...,\n",
              "         [-2.0083e-03, -1.7908e-03,  1.6512e-03,  ...,  1.4100e-03,\n",
              "          -2.2152e-03, -1.6832e-03],\n",
              "         [-2.3520e-03, -2.1258e-03, -8.2645e-04,  ..., -7.8081e-04,\n",
              "           1.9652e-03, -2.3219e-03],\n",
              "         [ 3.0959e-04,  1.7012e-03,  1.4480e-03,  ...,  8.2080e-04,\n",
              "          -6.2446e-04, -1.0509e-03]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-1.4708e-03, -2.4730e-03,  1.8678e-03, -7.8198e-04, -2.1158e-03,\n",
              "         -2.5081e-03, -1.3739e-04, -1.9671e-03, -8.5683e-04,  1.2015e-03,\n",
              "          2.0202e-03,  1.8313e-03,  1.2030e-03,  1.0304e-03,  1.1875e-03,\n",
              "         -1.0873e-03, -2.4109e-03, -1.1949e-03,  7.9937e-05,  1.2324e-03,\n",
              "         -1.6935e-03,  8.6058e-04,  2.8704e-04, -1.5699e-03, -4.2558e-04,\n",
              "          2.7178e-04,  1.8637e-03, -7.8755e-04, -5.6839e-06, -1.8082e-03,\n",
              "         -2.0806e-03,  5.5253e-04, -3.3672e-04,  1.5279e-03,  1.0672e-03,\n",
              "          1.6340e-03, -1.3704e-03, -1.5186e-03, -4.7138e-04, -1.1904e-03,\n",
              "          1.8236e-03, -1.9009e-03,  2.4820e-03,  2.3247e-03, -5.2619e-05,\n",
              "         -1.1823e-03,  1.9505e-03,  2.4244e-04, -1.9970e-03, -1.6927e-03,\n",
              "         -2.2694e-03,  2.5439e-03,  1.7157e-03, -2.6957e-04,  2.5466e-03,\n",
              "          2.3240e-03,  2.5141e-03, -1.6097e-03,  1.2506e-03,  1.1796e-03,\n",
              "         -3.7177e-04,  8.3519e-04, -2.3112e-03, -7.6658e-04, -4.2165e-04,\n",
              "         -1.1809e-03, -2.8993e-04, -1.6725e-03, -3.8804e-04,  1.1042e-03,\n",
              "          5.3143e-04, -1.1391e-03,  2.4807e-03,  2.1662e-04,  5.3259e-04,\n",
              "         -6.8878e-04,  2.2915e-03, -3.8492e-04,  2.3482e-03,  5.5071e-04,\n",
              "         -2.4576e-03,  9.2026e-04,  4.0075e-04, -8.1661e-04,  6.5107e-04,\n",
              "          7.1284e-04, -1.7637e-03, -7.8995e-04, -2.2599e-03,  2.0225e-03,\n",
              "          6.0292e-04, -2.5717e-03,  2.4016e-03,  8.4801e-04,  1.4025e-03,\n",
              "         -1.9957e-05, -1.6310e-04, -6.1351e-04,  7.2404e-04,  1.3024e-03,\n",
              "          5.2636e-04,  6.9235e-04,  2.5085e-03,  1.7794e-04, -1.0643e-03,\n",
              "         -1.7321e-03, -9.6266e-04, -8.1387e-04, -2.4822e-03, -9.1769e-04,\n",
              "          1.1968e-03,  1.9697e-03,  3.1331e-04, -8.3801e-04,  9.2181e-04,\n",
              "         -2.9958e-04, -2.0983e-03, -1.3601e-03, -4.6565e-04, -1.0013e-03,\n",
              "         -3.7971e-04,  2.4828e-03,  2.0162e-05,  1.8575e-03, -2.4637e-03,\n",
              "         -1.8833e-03,  1.8451e-03, -1.3049e-03,  2.2904e-03,  2.3794e-03,\n",
              "         -2.7041e-04,  1.9589e-03,  2.4414e-03, -1.6614e-03,  2.1852e-03,\n",
              "         -1.9707e-03, -6.4145e-04,  1.7375e-03, -2.0604e-03, -1.7961e-04,\n",
              "         -6.2863e-05,  3.8043e-04,  1.4640e-03,  7.8552e-04, -2.4699e-03,\n",
              "         -1.2760e-03, -6.2057e-04,  1.4582e-03, -1.0786e-03, -1.6756e-03,\n",
              "          9.7732e-04,  1.5883e-03,  8.5355e-04, -2.4935e-04, -9.7340e-04,\n",
              "          5.7725e-04,  1.3742e-03,  2.1146e-03,  1.5395e-03,  6.5642e-04,\n",
              "          1.2627e-03, -1.3945e-03,  1.1464e-03,  1.2325e-03, -2.4416e-03,\n",
              "          6.3576e-04, -1.5774e-03,  9.6073e-04, -1.5310e-03,  2.2904e-03,\n",
              "          2.1844e-03,  1.3974e-03, -4.7402e-04,  1.9482e-03, -5.1044e-05,\n",
              "         -2.4227e-03, -8.3418e-04, -5.0360e-04,  8.5833e-04, -5.2238e-04,\n",
              "          2.2434e-03, -2.3730e-04,  1.8311e-03, -9.8416e-05,  7.7538e-04,\n",
              "          1.0533e-03,  1.2368e-03, -1.1329e-03, -1.8063e-03, -9.2472e-04,\n",
              "          6.7873e-04,  3.5478e-04,  1.2808e-03, -2.0450e-03,  1.9660e-03,\n",
              "         -1.9363e-04, -1.5874e-03,  1.3642e-03, -1.5618e-03,  2.1166e-03,\n",
              "         -4.9252e-04, -1.0613e-03,  2.5338e-03,  1.4169e-05, -1.9318e-03,\n",
              "         -2.1969e-03, -7.0895e-05, -2.1250e-03, -1.7957e-03, -5.9636e-04,\n",
              "         -1.8702e-03, -8.2766e-04,  9.3344e-04,  2.1890e-03, -2.2079e-03,\n",
              "          1.9457e-03, -1.8609e-03, -2.1949e-03,  1.5549e-04, -2.3279e-03,\n",
              "         -2.1004e-03,  2.4426e-03, -4.2070e-04,  6.2140e-04, -2.2840e-03,\n",
              "          1.3624e-03,  1.1048e-03, -2.4945e-03,  2.3545e-03,  2.4499e-03,\n",
              "         -1.9115e-03,  2.1050e-03,  1.5823e-03,  2.3106e-03, -2.4976e-03,\n",
              "          6.4069e-04,  5.1516e-04, -1.2641e-03,  2.2189e-03, -8.7207e-05,\n",
              "         -5.8606e-05, -2.5688e-03, -2.5611e-03,  2.4781e-03, -1.3032e-03,\n",
              "          2.4872e-03,  2.2256e-03,  2.5419e-04,  1.6507e-03, -2.3306e-03,\n",
              "         -2.5191e-03,  6.3976e-04, -4.6101e-04,  1.5668e-03, -2.1740e-05,\n",
              "         -6.4161e-04,  1.4783e-03,  1.9157e-03,  1.5703e-03, -6.6847e-04,\n",
              "         -1.7813e-03,  1.0264e-03,  1.4619e-03,  1.5769e-03,  6.4528e-04,\n",
              "         -1.3137e-03, -2.2699e-03,  2.3901e-03,  2.0998e-03,  2.2048e-03,\n",
              "         -3.9575e-07,  3.2539e-04,  1.9808e-03,  2.2983e-03,  2.2212e-03,\n",
              "          2.3559e-03, -1.3626e-03, -1.9721e-03,  2.4664e-03,  2.2397e-03,\n",
              "         -1.5918e-03,  6.1229e-04,  2.6439e-04, -8.3441e-04, -2.4253e-03,\n",
              "         -2.2111e-03,  2.0541e-03, -2.5389e-03,  1.0545e-03,  7.9125e-04,\n",
              "          1.2062e-04, -1.1100e-03,  1.5035e-03, -1.7468e-03,  1.3347e-03,\n",
              "          1.9386e-03,  2.0086e-03,  1.5006e-03,  5.0614e-04, -9.8013e-04,\n",
              "          1.4955e-03,  1.5341e-03, -7.8514e-05, -5.1300e-04,  1.1464e-03,\n",
              "          1.2386e-03, -2.2851e-03,  2.2258e-04, -5.2396e-04, -2.1304e-03,\n",
              "         -1.3986e-03,  2.4240e-03, -7.1223e-05, -2.3002e-03, -8.3791e-04,\n",
              "         -4.9816e-04,  2.0271e-03, -1.3848e-03, -2.0559e-03,  2.1278e-03,\n",
              "         -2.3865e-03,  2.1749e-03, -1.7689e-03,  2.2160e-03,  1.6275e-03,\n",
              "         -1.1070e-03,  2.4071e-03, -2.5079e-03,  1.4595e-03, -2.4895e-03,\n",
              "          5.3013e-05, -4.1481e-04, -1.6202e-03,  9.7090e-04, -6.9478e-04,\n",
              "          2.4575e-03, -6.1803e-04,  2.0635e-03, -2.1938e-03, -2.0497e-03,\n",
              "         -2.3072e-03,  1.4050e-04, -9.0042e-04,  1.9799e-03, -4.8838e-04,\n",
              "          4.5062e-04, -2.1155e-03, -8.8869e-05,  1.8622e-03,  2.4374e-03,\n",
              "          1.7484e-03,  2.1601e-03,  1.2504e-03,  3.2834e-04,  8.1266e-05,\n",
              "          8.9859e-04,  4.7295e-04, -7.3122e-04, -1.6354e-03, -3.3177e-05,\n",
              "         -2.1499e-03,  1.3443e-03,  1.8726e-03,  1.8041e-03,  2.4671e-03,\n",
              "          3.2172e-04, -1.4230e-03, -8.4688e-04,  2.7651e-04,  3.3837e-04,\n",
              "          7.4615e-05, -2.1456e-03, -1.5544e-03, -2.4054e-03, -5.2053e-04,\n",
              "          1.4569e-03, -1.1389e-03,  5.5424e-04,  2.3679e-03, -1.3632e-03,\n",
              "          1.0685e-03, -8.2120e-04, -2.3559e-03,  9.4307e-04,  2.3000e-03,\n",
              "          1.1619e-03,  1.3354e-03, -1.6151e-03,  4.1218e-04, -9.2807e-04,\n",
              "          2.4126e-03, -2.5189e-03, -1.1369e-03, -2.1703e-03,  2.7006e-04,\n",
              "         -1.1833e-03, -2.3967e-03, -9.1315e-04, -5.4902e-04,  1.4343e-03,\n",
              "          8.1477e-04,  1.1610e-03, -4.5875e-04, -2.2175e-03, -1.6629e-03,\n",
              "         -2.0201e-03,  4.6653e-04, -7.7417e-04, -1.3982e-03,  3.4497e-04,\n",
              "         -8.9122e-04,  1.9567e-03, -2.0610e-03, -2.3038e-03, -3.3124e-04,\n",
              "          1.7749e-05, -2.2667e-03, -4.7612e-04, -1.4035e-03, -1.3199e-03,\n",
              "          2.7954e-04, -5.4426e-04,  6.5792e-04, -4.5676e-04, -4.8394e-04,\n",
              "          2.5623e-03,  1.8347e-03,  1.8067e-03,  4.5407e-04,  1.0201e-03,\n",
              "          4.8800e-05], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "print(model.linear.weight.shape, model.linear.bias.shape)\n",
        "list(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use our new custom model in the same way as before. Let's see if it works."
      ],
      "metadata": {
        "id": "zerUjdXBVSmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSJt4r3F2sT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d39b2e-d38a-446e-bd94-02d018c280af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 224, 224])\n",
            "outputs.shape :  torch.Size([32, 431])\n",
            "Sample outputs :\n",
            " tensor([[-2.6649e-01, -5.9991e-03,  2.0771e-01, -3.7419e-01, -1.0610e-01,\n",
            "         -7.0066e-01,  3.1469e-01, -1.9118e-01,  2.4316e-01,  2.5109e-02,\n",
            "         -7.9865e-01, -7.0293e-02, -7.8035e-01,  2.6730e-01, -5.4691e-01,\n",
            "         -1.6654e+00,  3.8568e-01, -3.0592e-01,  1.5885e+00, -2.1776e-01,\n",
            "          4.8043e-01, -8.9693e-01,  6.2812e-01, -8.0487e-01,  2.4533e-01,\n",
            "         -2.2773e-01, -4.9023e-01, -5.7397e-02, -7.1978e-02,  6.8417e-01,\n",
            "          3.0468e-01, -2.2018e-01,  1.7091e-01, -4.3361e-01, -1.3614e-01,\n",
            "         -5.2088e-01,  6.4118e-01, -5.3125e-01,  1.4665e+00,  1.5174e+00,\n",
            "          9.7967e-01,  5.0859e-01, -6.6972e-01,  1.9123e-01,  5.3745e-01,\n",
            "         -1.2128e-01,  7.6711e-02, -3.0778e-01, -4.6009e-02,  8.0425e-01,\n",
            "         -3.7026e-01,  1.5258e-01, -1.3606e+00, -3.5183e-01,  5.1647e-01,\n",
            "          1.1943e-01, -4.1414e-01, -1.8972e-01, -7.1182e-02,  4.3164e-01,\n",
            "         -1.3351e+00, -6.8693e-01,  1.0996e+00,  6.7960e-02, -9.8906e-02,\n",
            "          6.6148e-01,  1.6139e-01,  1.0055e-01, -4.1256e-01,  2.3737e-01,\n",
            "          1.1871e+00, -6.2013e-01, -1.7179e-01,  8.6545e-01, -1.9558e+00,\n",
            "         -4.5724e-01,  7.0063e-02, -1.1102e+00, -1.0603e+00, -7.0850e-01,\n",
            "         -3.2149e-01,  6.1530e-02,  2.6978e-01,  1.0194e+00,  6.3125e-01,\n",
            "         -5.0130e-01,  7.3629e-01,  8.3672e-01,  6.4149e-01, -1.3437e+00,\n",
            "         -3.4268e-01, -7.7218e-01,  7.9189e-01,  5.3550e-01,  6.0389e-02,\n",
            "         -4.3916e-01,  8.1574e-01,  3.5993e-01,  6.3937e-01, -1.0572e-01,\n",
            "         -1.0363e-01,  9.1991e-01,  4.5471e-01, -5.0576e-01,  9.2609e-01,\n",
            "         -1.6616e-01, -7.8292e-01, -3.1712e-01, -7.2174e-01,  9.4854e-02,\n",
            "          1.2569e+00,  8.3142e-01,  3.1027e-01, -5.5438e-01,  1.6920e-01,\n",
            "          1.3815e+00, -4.0292e-01, -5.4076e-01, -1.1493e-01,  3.8152e-02,\n",
            "          1.0589e+00,  1.4502e-01, -2.2963e-01,  6.4277e-01,  2.8132e-01,\n",
            "          7.9862e-01, -1.3653e-01, -9.6251e-01,  6.8168e-03,  2.7801e-02,\n",
            "         -1.1833e+00,  1.4858e-01,  1.0511e+00, -4.3835e-01, -2.3269e-01,\n",
            "          2.9281e-01, -3.9291e-01,  1.0341e+00, -3.5507e-01,  4.7759e-02,\n",
            "         -2.9888e-01, -5.4615e-01,  1.1440e+00, -4.4812e-01,  3.6858e-02,\n",
            "         -3.2022e-02, -4.7930e-01, -6.7097e-01,  1.1335e+00,  9.6531e-01,\n",
            "         -6.6374e-01, -7.5585e-01,  3.6964e-01, -6.2033e-01,  1.1047e+00,\n",
            "          9.3066e-02,  5.0421e-01, -4.1701e-01, -9.0490e-01, -8.4097e-01,\n",
            "          1.1018e-01,  7.5689e-01,  1.7275e-01, -7.6223e-01,  5.7520e-01,\n",
            "          4.2371e-02,  4.7680e-01, -7.0713e-01, -3.8828e-02,  4.2641e-01,\n",
            "          3.5636e-01,  1.1014e-01, -3.6608e-02,  3.1858e-01, -3.8282e-01,\n",
            "         -9.9895e-01,  1.2076e-01, -1.3374e+00,  7.0342e-01,  7.1355e-01,\n",
            "          1.2920e+00, -1.7174e-01, -4.9302e-02, -9.7964e-01, -1.2666e-02,\n",
            "          5.7145e-02,  1.9793e+00,  1.0565e-01, -4.7164e-01,  6.1177e-01,\n",
            "         -6.7420e-01,  2.4594e-02,  4.0145e-01,  5.0064e-01,  5.3819e-01,\n",
            "         -7.8765e-02, -2.2561e-01, -1.0589e+00, -6.7161e-01, -1.3370e+00,\n",
            "         -2.8852e-01, -1.0146e-01,  6.1145e-01, -8.7811e-01, -6.9947e-01,\n",
            "         -6.3487e-01, -5.9324e-01,  1.2261e+00, -1.5011e+00,  1.1196e+00,\n",
            "          2.3160e-01, -5.6532e-01,  3.3098e-01,  5.3730e-02,  9.3415e-01,\n",
            "         -1.6406e-01,  9.1403e-03,  4.8434e-02,  6.8331e-02,  9.4644e-01,\n",
            "          2.7792e-01,  3.1853e-01, -6.3992e-01,  6.2178e-01,  2.1019e-01,\n",
            "          2.3101e-01,  1.0277e+00,  1.2515e-02,  4.2150e-01, -7.2762e-01,\n",
            "          5.8563e-02, -4.5661e-01,  3.2690e-01,  3.1199e-01,  1.0504e-01,\n",
            "         -7.6822e-01, -1.0306e+00, -1.9351e-01, -4.2837e-01, -5.1585e-01,\n",
            "         -8.6734e-02, -2.5916e-01, -4.4342e-02,  3.0075e-01,  5.6710e-01,\n",
            "          1.4316e+00, -1.0461e+00,  4.0321e-01, -4.2482e-01,  3.9069e-01,\n",
            "         -5.3215e-02, -7.6812e-02, -8.1557e-01, -6.1453e-01, -3.0923e-01,\n",
            "          4.3192e-01, -5.1136e-01, -8.7456e-01,  1.9333e-01, -7.0170e-01,\n",
            "          1.7531e+00,  7.1061e-01, -5.3399e-01, -1.1258e-02, -1.4031e-01,\n",
            "         -4.0806e-01,  1.7729e-01, -7.3940e-01, -4.3006e-01,  1.4778e-01,\n",
            "         -7.6466e-01,  1.1463e+00,  2.9309e-01,  1.3241e-01,  6.7125e-01,\n",
            "         -1.8531e-01,  4.2409e-01,  9.2425e-02, -2.2919e-02,  3.3750e-02,\n",
            "         -6.0580e-01, -6.2436e-01,  4.3790e-01,  3.6833e-01,  6.2528e-01,\n",
            "         -1.4303e-01, -2.6318e-01, -3.8953e-01,  1.5864e-01, -1.3642e-01,\n",
            "         -5.9182e-01, -3.3539e-01,  1.3203e+00, -5.0096e-01,  1.5245e+00,\n",
            "          1.0805e-01, -1.2631e-02, -1.8749e-01, -5.2178e-01, -3.5947e-02,\n",
            "         -3.1489e-01,  9.6305e-02,  3.8667e-01, -3.5292e-01, -5.0717e-02,\n",
            "         -1.1000e-01,  7.6344e-01, -1.3826e-02, -2.7162e-01, -1.1048e-01,\n",
            "         -8.1743e-01, -9.0955e-01, -4.0890e-01, -1.3077e+00, -6.8516e-01,\n",
            "          3.4415e-01,  6.3110e-01,  3.9104e-02,  1.1239e+00,  3.4018e-01,\n",
            "          4.5994e-01, -2.9974e-01,  5.7424e-01, -1.2917e+00, -2.7770e-01,\n",
            "         -1.4407e+00,  4.9791e-01,  8.6339e-01, -2.2258e-02,  2.3699e-01,\n",
            "          7.2081e-02,  1.2197e-02,  1.7812e-01,  2.8017e-01, -2.0013e-01,\n",
            "         -1.0811e-01,  8.3850e-01, -5.0951e-02, -2.1956e-01, -3.4104e-01,\n",
            "          2.8042e-01,  7.4830e-01, -3.0467e-01, -2.1437e-02,  2.4523e-01,\n",
            "          1.9107e+00, -7.2158e-01, -1.7347e-01,  2.9271e-02,  6.6414e-01,\n",
            "         -3.7068e-01,  3.4662e-01, -1.3598e-01, -2.3390e-01,  7.1911e-02,\n",
            "         -3.4857e-01,  3.2390e-01,  4.5053e-01,  2.7539e-01,  4.2396e-01,\n",
            "         -3.3908e-01,  1.2414e-01, -4.7158e-02,  4.2349e-01,  2.0388e-01,\n",
            "         -2.8991e-01, -6.0664e-01, -1.0557e+00,  6.8169e-01,  1.4372e-01,\n",
            "          4.9542e-01,  5.2802e-02, -4.3358e-01, -1.0154e+00,  7.0501e-01,\n",
            "         -7.1561e-01,  1.4681e+00,  1.7612e-01,  7.3543e-01,  3.1453e-01,\n",
            "         -1.7526e-01, -2.6634e-01,  1.2080e+00, -3.5939e-01, -1.0774e+00,\n",
            "         -5.3320e-01, -9.6798e-01, -2.1823e-01, -2.2483e-02, -1.6394e+00,\n",
            "          5.4933e-02, -1.5625e-01, -7.1890e-01, -5.1505e-01,  9.7430e-02,\n",
            "         -5.8235e-01, -3.7881e-01,  1.5534e-01,  2.9725e-01, -4.5980e-01,\n",
            "         -1.4106e+00,  3.5009e-01,  2.9507e-01,  8.0823e-01,  1.4402e+00,\n",
            "         -4.7553e-01,  5.9858e-01, -6.6094e-01,  7.9237e-01, -1.5184e+00,\n",
            "         -4.7851e-01, -5.7514e-01,  1.7030e+00, -4.1418e-01,  4.4532e-01,\n",
            "          9.1360e-01, -2.6257e-01,  1.0812e+00,  3.8779e-01,  1.0327e+00,\n",
            "          7.5560e-01,  7.1686e-01, -3.6724e-01, -9.5899e-01,  1.9424e-01,\n",
            "         -1.8445e-01,  3.9652e-01, -9.1429e-04, -1.4303e+00, -3.2508e-01,\n",
            "         -1.0903e+00],\n",
            "        [ 2.5603e-01, -7.1324e-01, -7.1967e-01, -8.4554e-01, -6.4591e-01,\n",
            "          1.3197e+00,  7.6177e-01,  2.6632e-01,  4.3446e-01, -7.9389e-01,\n",
            "          7.2788e-01,  3.5445e-01, -3.7735e-01,  3.3504e-01,  9.3321e-01,\n",
            "         -2.2945e-01,  1.6167e-01,  1.3383e-02, -9.1503e-01, -1.1522e-01,\n",
            "         -6.1435e-01,  1.8199e+00,  6.6777e-01,  1.1592e+00, -1.0737e-01,\n",
            "          5.4494e-01, -1.2062e+00,  1.5319e+00, -1.4490e-01, -7.2997e-01,\n",
            "         -1.5604e+00, -5.0435e-01,  3.2512e-01,  6.5534e-02, -9.1867e-01,\n",
            "          2.7986e-01, -1.5058e+00,  7.3777e-01, -1.3063e+00, -1.1860e+00,\n",
            "         -1.3438e-01,  1.3672e-03,  8.8443e-01,  7.1328e-02, -6.4961e-01,\n",
            "          1.1508e+00, -3.7876e-01, -1.0950e+00, -1.4732e+00,  9.0619e-01,\n",
            "         -1.2413e+00, -5.0067e-01, -5.2251e-01,  4.3243e-01,  1.1773e+00,\n",
            "          3.7435e-01, -5.6747e-01, -5.6075e-01, -4.4487e-01, -9.4211e-01,\n",
            "          1.3886e+00,  6.1655e-01, -2.9219e-02, -6.9411e-01,  1.5772e+00,\n",
            "         -2.4705e-01, -2.0996e+00, -9.5644e-01,  7.8392e-01,  3.7212e-01,\n",
            "          1.1443e+00, -9.1701e-02,  3.0563e-01, -1.1534e+00,  3.0504e-01,\n",
            "         -3.3520e-01,  1.2870e+00, -3.9834e-01, -1.0124e+00,  7.4552e-01,\n",
            "          7.0079e-01, -1.3062e-01,  2.6059e-01, -1.1108e+00, -2.4825e+00,\n",
            "         -6.9267e-01, -8.9930e-01, -7.5877e-01, -8.0919e-01,  3.0447e-01,\n",
            "         -3.3349e-01,  2.6946e-01,  5.7015e-01,  8.6994e-01, -7.2145e-01,\n",
            "          8.7883e-01, -1.4392e+00,  1.5426e-01, -7.9254e-01,  1.9998e-01,\n",
            "          8.5024e-01, -2.6332e-01,  1.2069e+00,  1.3135e-01, -8.3295e-02,\n",
            "         -1.6019e+00,  1.0831e+00, -4.1808e-01,  1.3611e+00, -5.6642e-01,\n",
            "         -1.1248e+00, -2.6765e-01,  1.2137e+00, -8.9705e-01, -6.1245e-01,\n",
            "         -1.3998e+00, -8.7452e-01, -1.2425e+00,  1.2775e+00,  1.0462e+00,\n",
            "          2.2433e-02, -1.3156e-01,  9.4560e-02,  5.2234e-01, -1.1868e+00,\n",
            "         -2.1071e-01,  7.9099e-01, -9.5959e-01, -5.5506e-01,  4.7540e-01,\n",
            "         -5.6722e-01,  4.8215e-01, -3.9175e-01, -2.5481e-01,  5.2872e-01,\n",
            "         -6.2592e-02,  9.0851e-01, -1.1996e+00,  4.4784e-01, -3.8398e-01,\n",
            "          7.3516e-02,  6.9608e-01, -6.5399e-01,  9.1876e-01,  1.0456e+00,\n",
            "         -4.1964e-01,  3.6172e-01,  9.2633e-01, -2.7194e-01, -3.4964e-01,\n",
            "         -8.7674e-01,  6.4280e-01,  2.9639e-01, -1.2666e+00,  5.2152e-01,\n",
            "          7.8229e-01, -8.6223e-01,  2.1443e+00,  1.8443e+00,  6.5370e-01,\n",
            "          1.4375e-01, -1.1078e+00,  2.5774e-01,  1.4006e+00, -6.0557e-02,\n",
            "         -6.0645e-01, -5.6105e-01, -4.3706e-02, -6.1488e-02,  2.2302e-01,\n",
            "         -8.1128e-01,  8.4881e-01,  9.8719e-01,  6.6740e-02, -4.3942e-01,\n",
            "          6.0916e-01, -1.0401e+00, -1.6904e-01, -7.9668e-01,  2.8153e-01,\n",
            "          1.3239e+00,  1.0178e+00, -5.1435e-01,  1.4125e+00,  2.9240e-01,\n",
            "          4.4670e-01, -5.4484e-01,  4.2875e-01, -3.3974e-01,  3.8248e-01,\n",
            "          4.9644e-01, -1.0042e+00, -1.7849e+00, -3.7511e-01,  1.5846e+00,\n",
            "         -3.7804e-03,  4.9275e-02,  4.0950e-01,  1.0768e+00, -7.0005e-01,\n",
            "          1.4701e+00,  1.7109e-01, -1.9529e-01, -3.4588e-01,  3.8041e-01,\n",
            "         -3.3773e-01, -1.1314e+00, -2.4053e+00,  6.6326e-01, -8.9633e-01,\n",
            "          1.2384e+00,  1.5494e-01, -5.1893e-01,  1.8940e-02,  7.7267e-01,\n",
            "          1.0871e-01,  5.6111e-01,  1.2630e+00, -7.6388e-01, -5.8697e-03,\n",
            "         -3.6886e-02, -1.3012e+00, -6.2903e-01, -9.4983e-01,  1.2121e+00,\n",
            "         -7.7395e-01, -7.9955e-01, -2.4683e-02, -6.2256e-01, -1.2200e+00,\n",
            "         -3.2801e-01, -5.5645e-01,  5.4229e-01, -2.9409e-01, -7.3464e-01,\n",
            "          1.9392e-01, -1.0395e-01, -1.1647e+00, -9.9294e-03, -2.8010e-02,\n",
            "         -8.5605e-01,  6.6953e-01,  5.9072e-01,  2.7137e-01,  1.7121e-01,\n",
            "         -9.0983e-01,  3.6251e-01, -2.6866e-01, -6.6471e-01,  6.2880e-01,\n",
            "         -4.2859e-01,  3.2046e-02,  2.2932e-01,  5.2863e-01,  2.1417e-01,\n",
            "         -2.8897e-01,  2.4733e-01,  4.8851e-02, -6.4772e-01, -1.0839e+00,\n",
            "         -5.1585e-01,  6.7840e-01, -5.3798e-01, -6.7101e-01,  1.2739e+00,\n",
            "          6.9670e-02,  1.0125e-02, -5.2771e-01, -9.6526e-01,  1.2259e-01,\n",
            "          4.3712e-01, -5.5094e-01, -1.6795e+00, -1.5128e+00, -4.8495e-01,\n",
            "         -1.5389e+00, -9.9888e-01, -9.1453e-02,  1.5259e+00,  1.1483e+00,\n",
            "          1.1097e+00,  9.5077e-01,  6.7717e-01, -5.1033e-01, -1.2239e+00,\n",
            "          9.2210e-01, -1.4161e+00,  1.4811e+00, -1.7611e-01,  8.9202e-01,\n",
            "          4.7485e-01, -5.2845e-01,  1.2280e-01,  1.7376e+00,  8.1058e-01,\n",
            "         -9.6744e-01, -5.3657e-01,  1.0842e+00, -7.4364e-01,  2.8135e-01,\n",
            "          4.3070e-01,  1.6902e+00, -2.6781e-01,  2.3456e-01, -1.4908e+00,\n",
            "         -1.6727e+00,  9.7076e-03,  1.8494e-01, -9.2255e-01, -1.0548e+00,\n",
            "          7.3956e-01, -7.6959e-02, -1.1083e+00, -3.0063e-01,  3.5735e-01,\n",
            "         -7.4811e-02,  1.1821e+00, -3.0897e-01, -1.2252e-01,  8.8708e-01,\n",
            "         -1.3590e+00, -6.8218e-01,  1.0654e-01, -1.8007e-01,  1.5150e+00,\n",
            "         -4.1820e-01, -5.5596e-01,  1.2887e+00, -4.4786e-01,  9.3518e-01,\n",
            "          7.8680e-01,  5.3313e-01, -1.5024e-01,  1.2865e+00,  1.0876e+00,\n",
            "          1.3063e-01, -3.0401e-02,  1.0846e+00,  1.1851e+00, -4.2530e-01,\n",
            "          8.8967e-01,  1.1379e-01,  7.7512e-01,  8.1924e-01, -6.4804e-01,\n",
            "         -3.2721e-01, -3.8847e-01,  1.1013e+00, -6.1011e-01,  6.8301e-01,\n",
            "         -5.6665e-01,  2.6738e-01, -9.1218e-01,  5.0870e-01, -5.8891e-01,\n",
            "          1.1564e+00,  3.8836e-01, -5.8187e-01,  2.7406e-01, -1.8718e-01,\n",
            "         -1.0160e+00, -2.8389e-01, -1.9977e-01, -2.5817e-01,  1.6194e-01,\n",
            "          4.4441e-01, -4.8075e-01, -3.5324e-01,  2.3039e-01,  2.8011e-02,\n",
            "          5.2083e-01, -5.3109e-01, -1.6585e+00, -5.4438e-01,  4.3566e-01,\n",
            "         -1.2875e-01, -2.0025e-01, -2.7136e-01, -1.1682e-02, -1.7767e+00,\n",
            "          6.8691e-01, -4.9904e-01, -1.4772e+00,  1.6936e+00, -1.1119e+00,\n",
            "         -3.3611e-01,  1.6016e-01, -2.4432e-01, -3.9504e-01,  8.9875e-01,\n",
            "          2.9992e-01,  1.1382e+00,  4.4606e-01,  2.7185e+00, -1.0644e+00,\n",
            "          1.0307e+00, -3.1823e-01,  9.0443e-01, -3.1055e-01, -1.2264e+00,\n",
            "         -4.9938e-01,  1.0359e+00,  1.4753e+00, -2.6225e-01, -1.3580e+00,\n",
            "          1.0898e+00, -5.0837e-02,  5.2203e-02, -9.9044e-01,  6.7885e-01,\n",
            "          1.0061e+00, -7.1469e-01,  1.2832e+00,  2.1572e-01, -4.3626e-01,\n",
            "         -1.0389e+00, -5.3543e-01,  5.3068e-01, -1.7765e+00, -2.7684e-01,\n",
            "          2.9175e-01, -1.6760e-01,  7.0996e-01,  1.8965e+00, -1.0857e-01,\n",
            "          7.1688e-01,  9.2957e-01, -5.1933e-01,  4.6385e-01,  1.3670e+00,\n",
            "         -3.4656e-01]])\n"
          ]
        }
      ],
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    outputs = model(images)\n",
        "    break\n",
        "\n",
        "print('outputs.shape : ', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:2].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of the 100 input images, we get 431 outputs, one for each class. We'd like these outputs to represent probabilities. Each output row's elements must lie between 0 to 1 and add up to 1, which is not the case. \n",
        "\n",
        "To convert the output rows into probabilities, we use the softmax function, which has the following formula:\n",
        "\n",
        "![softmax](https://i.imgur.com/EAh9jLN.png)\n",
        "\n",
        "First, we replace each element `yi` in an output row by `e^yi`, making all the elements positive. \n",
        "\n",
        "![](https://www.montereyinstitute.org/courses/DevelopmentalMath/COURSE_TEXT2_RESOURCE/U18_L1_T1_text_final_6_files/image001.png)\n",
        "\n",
        "\n",
        "\n",
        "Then, we divide them by their sum to ensure that they add up to 1. The resulting vector can thus be interpreted as probabilities.\n",
        "\n",
        "While it's easy to implement the softmax function, we'll use the implementation that's provided within PyTorch because it works well with multidimensional tensors (a list of output rows in our case)."
      ],
      "metadata": {
        "id": "U3FBOawXVdEC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZzPOOec3YeZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax function is included in the `torch.nn.functional` package and requires us to specify a dimension along which the function should be applied."
      ],
      "metadata": {
        "id": "rXg5ON2zVtpe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XMWwlTY3cHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab091e88-93bc-4a35-d5c5-af6427da3cc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.6649e-01, -5.9991e-03,  2.0771e-01, -3.7419e-01, -1.0610e-01,\n",
              "         -7.0066e-01,  3.1469e-01, -1.9118e-01,  2.4316e-01,  2.5109e-02,\n",
              "         -7.9865e-01, -7.0293e-02, -7.8035e-01,  2.6730e-01, -5.4691e-01,\n",
              "         -1.6654e+00,  3.8568e-01, -3.0592e-01,  1.5885e+00, -2.1776e-01,\n",
              "          4.8043e-01, -8.9693e-01,  6.2812e-01, -8.0487e-01,  2.4533e-01,\n",
              "         -2.2773e-01, -4.9023e-01, -5.7397e-02, -7.1978e-02,  6.8417e-01,\n",
              "          3.0468e-01, -2.2018e-01,  1.7091e-01, -4.3361e-01, -1.3614e-01,\n",
              "         -5.2088e-01,  6.4118e-01, -5.3125e-01,  1.4665e+00,  1.5174e+00,\n",
              "          9.7967e-01,  5.0859e-01, -6.6972e-01,  1.9123e-01,  5.3745e-01,\n",
              "         -1.2128e-01,  7.6711e-02, -3.0778e-01, -4.6009e-02,  8.0425e-01,\n",
              "         -3.7026e-01,  1.5258e-01, -1.3606e+00, -3.5183e-01,  5.1647e-01,\n",
              "          1.1943e-01, -4.1414e-01, -1.8972e-01, -7.1182e-02,  4.3164e-01,\n",
              "         -1.3351e+00, -6.8693e-01,  1.0996e+00,  6.7960e-02, -9.8906e-02,\n",
              "          6.6148e-01,  1.6139e-01,  1.0055e-01, -4.1256e-01,  2.3737e-01,\n",
              "          1.1871e+00, -6.2013e-01, -1.7179e-01,  8.6545e-01, -1.9558e+00,\n",
              "         -4.5724e-01,  7.0063e-02, -1.1102e+00, -1.0603e+00, -7.0850e-01,\n",
              "         -3.2149e-01,  6.1530e-02,  2.6978e-01,  1.0194e+00,  6.3125e-01,\n",
              "         -5.0130e-01,  7.3629e-01,  8.3672e-01,  6.4149e-01, -1.3437e+00,\n",
              "         -3.4268e-01, -7.7218e-01,  7.9189e-01,  5.3550e-01,  6.0389e-02,\n",
              "         -4.3916e-01,  8.1574e-01,  3.5993e-01,  6.3937e-01, -1.0572e-01,\n",
              "         -1.0363e-01,  9.1991e-01,  4.5471e-01, -5.0576e-01,  9.2609e-01,\n",
              "         -1.6616e-01, -7.8292e-01, -3.1712e-01, -7.2174e-01,  9.4854e-02,\n",
              "          1.2569e+00,  8.3142e-01,  3.1027e-01, -5.5438e-01,  1.6920e-01,\n",
              "          1.3815e+00, -4.0292e-01, -5.4076e-01, -1.1493e-01,  3.8152e-02,\n",
              "          1.0589e+00,  1.4502e-01, -2.2963e-01,  6.4277e-01,  2.8132e-01,\n",
              "          7.9862e-01, -1.3653e-01, -9.6251e-01,  6.8168e-03,  2.7801e-02,\n",
              "         -1.1833e+00,  1.4858e-01,  1.0511e+00, -4.3835e-01, -2.3269e-01,\n",
              "          2.9281e-01, -3.9291e-01,  1.0341e+00, -3.5507e-01,  4.7759e-02,\n",
              "         -2.9888e-01, -5.4615e-01,  1.1440e+00, -4.4812e-01,  3.6858e-02,\n",
              "         -3.2022e-02, -4.7930e-01, -6.7097e-01,  1.1335e+00,  9.6531e-01,\n",
              "         -6.6374e-01, -7.5585e-01,  3.6964e-01, -6.2033e-01,  1.1047e+00,\n",
              "          9.3066e-02,  5.0421e-01, -4.1701e-01, -9.0490e-01, -8.4097e-01,\n",
              "          1.1018e-01,  7.5689e-01,  1.7275e-01, -7.6223e-01,  5.7520e-01,\n",
              "          4.2371e-02,  4.7680e-01, -7.0713e-01, -3.8828e-02,  4.2641e-01,\n",
              "          3.5636e-01,  1.1014e-01, -3.6608e-02,  3.1858e-01, -3.8282e-01,\n",
              "         -9.9895e-01,  1.2076e-01, -1.3374e+00,  7.0342e-01,  7.1355e-01,\n",
              "          1.2920e+00, -1.7174e-01, -4.9302e-02, -9.7964e-01, -1.2666e-02,\n",
              "          5.7145e-02,  1.9793e+00,  1.0565e-01, -4.7164e-01,  6.1177e-01,\n",
              "         -6.7420e-01,  2.4594e-02,  4.0145e-01,  5.0064e-01,  5.3819e-01,\n",
              "         -7.8765e-02, -2.2561e-01, -1.0589e+00, -6.7161e-01, -1.3370e+00,\n",
              "         -2.8852e-01, -1.0146e-01,  6.1145e-01, -8.7811e-01, -6.9947e-01,\n",
              "         -6.3487e-01, -5.9324e-01,  1.2261e+00, -1.5011e+00,  1.1196e+00,\n",
              "          2.3160e-01, -5.6532e-01,  3.3098e-01,  5.3730e-02,  9.3415e-01,\n",
              "         -1.6406e-01,  9.1403e-03,  4.8434e-02,  6.8331e-02,  9.4644e-01,\n",
              "          2.7792e-01,  3.1853e-01, -6.3992e-01,  6.2178e-01,  2.1019e-01,\n",
              "          2.3101e-01,  1.0277e+00,  1.2515e-02,  4.2150e-01, -7.2762e-01,\n",
              "          5.8563e-02, -4.5661e-01,  3.2690e-01,  3.1199e-01,  1.0504e-01,\n",
              "         -7.6822e-01, -1.0306e+00, -1.9351e-01, -4.2837e-01, -5.1585e-01,\n",
              "         -8.6734e-02, -2.5916e-01, -4.4342e-02,  3.0075e-01,  5.6710e-01,\n",
              "          1.4316e+00, -1.0461e+00,  4.0321e-01, -4.2482e-01,  3.9069e-01,\n",
              "         -5.3215e-02, -7.6812e-02, -8.1557e-01, -6.1453e-01, -3.0923e-01,\n",
              "          4.3192e-01, -5.1136e-01, -8.7456e-01,  1.9333e-01, -7.0170e-01,\n",
              "          1.7531e+00,  7.1061e-01, -5.3399e-01, -1.1258e-02, -1.4031e-01,\n",
              "         -4.0806e-01,  1.7729e-01, -7.3940e-01, -4.3006e-01,  1.4778e-01,\n",
              "         -7.6466e-01,  1.1463e+00,  2.9309e-01,  1.3241e-01,  6.7125e-01,\n",
              "         -1.8531e-01,  4.2409e-01,  9.2425e-02, -2.2919e-02,  3.3750e-02,\n",
              "         -6.0580e-01, -6.2436e-01,  4.3790e-01,  3.6833e-01,  6.2528e-01,\n",
              "         -1.4303e-01, -2.6318e-01, -3.8953e-01,  1.5864e-01, -1.3642e-01,\n",
              "         -5.9182e-01, -3.3539e-01,  1.3203e+00, -5.0096e-01,  1.5245e+00,\n",
              "          1.0805e-01, -1.2631e-02, -1.8749e-01, -5.2178e-01, -3.5947e-02,\n",
              "         -3.1489e-01,  9.6305e-02,  3.8667e-01, -3.5292e-01, -5.0717e-02,\n",
              "         -1.1000e-01,  7.6344e-01, -1.3826e-02, -2.7162e-01, -1.1048e-01,\n",
              "         -8.1743e-01, -9.0955e-01, -4.0890e-01, -1.3077e+00, -6.8516e-01,\n",
              "          3.4415e-01,  6.3110e-01,  3.9104e-02,  1.1239e+00,  3.4018e-01,\n",
              "          4.5994e-01, -2.9974e-01,  5.7424e-01, -1.2917e+00, -2.7770e-01,\n",
              "         -1.4407e+00,  4.9791e-01,  8.6339e-01, -2.2258e-02,  2.3699e-01,\n",
              "          7.2081e-02,  1.2197e-02,  1.7812e-01,  2.8017e-01, -2.0013e-01,\n",
              "         -1.0811e-01,  8.3850e-01, -5.0951e-02, -2.1956e-01, -3.4104e-01,\n",
              "          2.8042e-01,  7.4830e-01, -3.0467e-01, -2.1437e-02,  2.4523e-01,\n",
              "          1.9107e+00, -7.2158e-01, -1.7347e-01,  2.9271e-02,  6.6414e-01,\n",
              "         -3.7068e-01,  3.4662e-01, -1.3598e-01, -2.3390e-01,  7.1911e-02,\n",
              "         -3.4857e-01,  3.2390e-01,  4.5053e-01,  2.7539e-01,  4.2396e-01,\n",
              "         -3.3908e-01,  1.2414e-01, -4.7158e-02,  4.2349e-01,  2.0388e-01,\n",
              "         -2.8991e-01, -6.0664e-01, -1.0557e+00,  6.8169e-01,  1.4372e-01,\n",
              "          4.9542e-01,  5.2802e-02, -4.3358e-01, -1.0154e+00,  7.0501e-01,\n",
              "         -7.1561e-01,  1.4681e+00,  1.7612e-01,  7.3543e-01,  3.1453e-01,\n",
              "         -1.7526e-01, -2.6634e-01,  1.2080e+00, -3.5939e-01, -1.0774e+00,\n",
              "         -5.3320e-01, -9.6798e-01, -2.1823e-01, -2.2483e-02, -1.6394e+00,\n",
              "          5.4933e-02, -1.5625e-01, -7.1890e-01, -5.1505e-01,  9.7430e-02,\n",
              "         -5.8235e-01, -3.7881e-01,  1.5534e-01,  2.9725e-01, -4.5980e-01,\n",
              "         -1.4106e+00,  3.5009e-01,  2.9507e-01,  8.0823e-01,  1.4402e+00,\n",
              "         -4.7553e-01,  5.9858e-01, -6.6094e-01,  7.9237e-01, -1.5184e+00,\n",
              "         -4.7851e-01, -5.7514e-01,  1.7030e+00, -4.1418e-01,  4.4532e-01,\n",
              "          9.1360e-01, -2.6257e-01,  1.0812e+00,  3.8779e-01,  1.0327e+00,\n",
              "          7.5560e-01,  7.1686e-01, -3.6724e-01, -9.5899e-01,  1.9424e-01,\n",
              "         -1.8445e-01,  3.9652e-01, -9.1429e-04, -1.4303e+00, -3.2508e-01,\n",
              "         -1.0903e+00],\n",
              "        [ 2.5603e-01, -7.1324e-01, -7.1967e-01, -8.4554e-01, -6.4591e-01,\n",
              "          1.3197e+00,  7.6177e-01,  2.6632e-01,  4.3446e-01, -7.9389e-01,\n",
              "          7.2788e-01,  3.5445e-01, -3.7735e-01,  3.3504e-01,  9.3321e-01,\n",
              "         -2.2945e-01,  1.6167e-01,  1.3383e-02, -9.1503e-01, -1.1522e-01,\n",
              "         -6.1435e-01,  1.8199e+00,  6.6777e-01,  1.1592e+00, -1.0737e-01,\n",
              "          5.4494e-01, -1.2062e+00,  1.5319e+00, -1.4490e-01, -7.2997e-01,\n",
              "         -1.5604e+00, -5.0435e-01,  3.2512e-01,  6.5534e-02, -9.1867e-01,\n",
              "          2.7986e-01, -1.5058e+00,  7.3777e-01, -1.3063e+00, -1.1860e+00,\n",
              "         -1.3438e-01,  1.3672e-03,  8.8443e-01,  7.1328e-02, -6.4961e-01,\n",
              "          1.1508e+00, -3.7876e-01, -1.0950e+00, -1.4732e+00,  9.0619e-01,\n",
              "         -1.2413e+00, -5.0067e-01, -5.2251e-01,  4.3243e-01,  1.1773e+00,\n",
              "          3.7435e-01, -5.6747e-01, -5.6075e-01, -4.4487e-01, -9.4211e-01,\n",
              "          1.3886e+00,  6.1655e-01, -2.9219e-02, -6.9411e-01,  1.5772e+00,\n",
              "         -2.4705e-01, -2.0996e+00, -9.5644e-01,  7.8392e-01,  3.7212e-01,\n",
              "          1.1443e+00, -9.1701e-02,  3.0563e-01, -1.1534e+00,  3.0504e-01,\n",
              "         -3.3520e-01,  1.2870e+00, -3.9834e-01, -1.0124e+00,  7.4552e-01,\n",
              "          7.0079e-01, -1.3062e-01,  2.6059e-01, -1.1108e+00, -2.4825e+00,\n",
              "         -6.9267e-01, -8.9930e-01, -7.5877e-01, -8.0919e-01,  3.0447e-01,\n",
              "         -3.3349e-01,  2.6946e-01,  5.7015e-01,  8.6994e-01, -7.2145e-01,\n",
              "          8.7883e-01, -1.4392e+00,  1.5426e-01, -7.9254e-01,  1.9998e-01,\n",
              "          8.5024e-01, -2.6332e-01,  1.2069e+00,  1.3135e-01, -8.3295e-02,\n",
              "         -1.6019e+00,  1.0831e+00, -4.1808e-01,  1.3611e+00, -5.6642e-01,\n",
              "         -1.1248e+00, -2.6765e-01,  1.2137e+00, -8.9705e-01, -6.1245e-01,\n",
              "         -1.3998e+00, -8.7452e-01, -1.2425e+00,  1.2775e+00,  1.0462e+00,\n",
              "          2.2433e-02, -1.3156e-01,  9.4560e-02,  5.2234e-01, -1.1868e+00,\n",
              "         -2.1071e-01,  7.9099e-01, -9.5959e-01, -5.5506e-01,  4.7540e-01,\n",
              "         -5.6722e-01,  4.8215e-01, -3.9175e-01, -2.5481e-01,  5.2872e-01,\n",
              "         -6.2592e-02,  9.0851e-01, -1.1996e+00,  4.4784e-01, -3.8398e-01,\n",
              "          7.3516e-02,  6.9608e-01, -6.5399e-01,  9.1876e-01,  1.0456e+00,\n",
              "         -4.1964e-01,  3.6172e-01,  9.2633e-01, -2.7194e-01, -3.4964e-01,\n",
              "         -8.7674e-01,  6.4280e-01,  2.9639e-01, -1.2666e+00,  5.2152e-01,\n",
              "          7.8229e-01, -8.6223e-01,  2.1443e+00,  1.8443e+00,  6.5370e-01,\n",
              "          1.4375e-01, -1.1078e+00,  2.5774e-01,  1.4006e+00, -6.0557e-02,\n",
              "         -6.0645e-01, -5.6105e-01, -4.3706e-02, -6.1488e-02,  2.2302e-01,\n",
              "         -8.1128e-01,  8.4881e-01,  9.8719e-01,  6.6740e-02, -4.3942e-01,\n",
              "          6.0916e-01, -1.0401e+00, -1.6904e-01, -7.9668e-01,  2.8153e-01,\n",
              "          1.3239e+00,  1.0178e+00, -5.1435e-01,  1.4125e+00,  2.9240e-01,\n",
              "          4.4670e-01, -5.4484e-01,  4.2875e-01, -3.3974e-01,  3.8248e-01,\n",
              "          4.9644e-01, -1.0042e+00, -1.7849e+00, -3.7511e-01,  1.5846e+00,\n",
              "         -3.7804e-03,  4.9275e-02,  4.0950e-01,  1.0768e+00, -7.0005e-01,\n",
              "          1.4701e+00,  1.7109e-01, -1.9529e-01, -3.4588e-01,  3.8041e-01,\n",
              "         -3.3773e-01, -1.1314e+00, -2.4053e+00,  6.6326e-01, -8.9633e-01,\n",
              "          1.2384e+00,  1.5494e-01, -5.1893e-01,  1.8940e-02,  7.7267e-01,\n",
              "          1.0871e-01,  5.6111e-01,  1.2630e+00, -7.6388e-01, -5.8697e-03,\n",
              "         -3.6886e-02, -1.3012e+00, -6.2903e-01, -9.4983e-01,  1.2121e+00,\n",
              "         -7.7395e-01, -7.9955e-01, -2.4683e-02, -6.2256e-01, -1.2200e+00,\n",
              "         -3.2801e-01, -5.5645e-01,  5.4229e-01, -2.9409e-01, -7.3464e-01,\n",
              "          1.9392e-01, -1.0395e-01, -1.1647e+00, -9.9294e-03, -2.8010e-02,\n",
              "         -8.5605e-01,  6.6953e-01,  5.9072e-01,  2.7137e-01,  1.7121e-01,\n",
              "         -9.0983e-01,  3.6251e-01, -2.6866e-01, -6.6471e-01,  6.2880e-01,\n",
              "         -4.2859e-01,  3.2046e-02,  2.2932e-01,  5.2863e-01,  2.1417e-01,\n",
              "         -2.8897e-01,  2.4733e-01,  4.8851e-02, -6.4772e-01, -1.0839e+00,\n",
              "         -5.1585e-01,  6.7840e-01, -5.3798e-01, -6.7101e-01,  1.2739e+00,\n",
              "          6.9670e-02,  1.0125e-02, -5.2771e-01, -9.6526e-01,  1.2259e-01,\n",
              "          4.3712e-01, -5.5094e-01, -1.6795e+00, -1.5128e+00, -4.8495e-01,\n",
              "         -1.5389e+00, -9.9888e-01, -9.1453e-02,  1.5259e+00,  1.1483e+00,\n",
              "          1.1097e+00,  9.5077e-01,  6.7717e-01, -5.1033e-01, -1.2239e+00,\n",
              "          9.2210e-01, -1.4161e+00,  1.4811e+00, -1.7611e-01,  8.9202e-01,\n",
              "          4.7485e-01, -5.2845e-01,  1.2280e-01,  1.7376e+00,  8.1058e-01,\n",
              "         -9.6744e-01, -5.3657e-01,  1.0842e+00, -7.4364e-01,  2.8135e-01,\n",
              "          4.3070e-01,  1.6902e+00, -2.6781e-01,  2.3456e-01, -1.4908e+00,\n",
              "         -1.6727e+00,  9.7076e-03,  1.8494e-01, -9.2255e-01, -1.0548e+00,\n",
              "          7.3956e-01, -7.6959e-02, -1.1083e+00, -3.0063e-01,  3.5735e-01,\n",
              "         -7.4811e-02,  1.1821e+00, -3.0897e-01, -1.2252e-01,  8.8708e-01,\n",
              "         -1.3590e+00, -6.8218e-01,  1.0654e-01, -1.8007e-01,  1.5150e+00,\n",
              "         -4.1820e-01, -5.5596e-01,  1.2887e+00, -4.4786e-01,  9.3518e-01,\n",
              "          7.8680e-01,  5.3313e-01, -1.5024e-01,  1.2865e+00,  1.0876e+00,\n",
              "          1.3063e-01, -3.0401e-02,  1.0846e+00,  1.1851e+00, -4.2530e-01,\n",
              "          8.8967e-01,  1.1379e-01,  7.7512e-01,  8.1924e-01, -6.4804e-01,\n",
              "         -3.2721e-01, -3.8847e-01,  1.1013e+00, -6.1011e-01,  6.8301e-01,\n",
              "         -5.6665e-01,  2.6738e-01, -9.1218e-01,  5.0870e-01, -5.8891e-01,\n",
              "          1.1564e+00,  3.8836e-01, -5.8187e-01,  2.7406e-01, -1.8718e-01,\n",
              "         -1.0160e+00, -2.8389e-01, -1.9977e-01, -2.5817e-01,  1.6194e-01,\n",
              "          4.4441e-01, -4.8075e-01, -3.5324e-01,  2.3039e-01,  2.8011e-02,\n",
              "          5.2083e-01, -5.3109e-01, -1.6585e+00, -5.4438e-01,  4.3566e-01,\n",
              "         -1.2875e-01, -2.0025e-01, -2.7136e-01, -1.1682e-02, -1.7767e+00,\n",
              "          6.8691e-01, -4.9904e-01, -1.4772e+00,  1.6936e+00, -1.1119e+00,\n",
              "         -3.3611e-01,  1.6016e-01, -2.4432e-01, -3.9504e-01,  8.9875e-01,\n",
              "          2.9992e-01,  1.1382e+00,  4.4606e-01,  2.7185e+00, -1.0644e+00,\n",
              "          1.0307e+00, -3.1823e-01,  9.0443e-01, -3.1055e-01, -1.2264e+00,\n",
              "         -4.9938e-01,  1.0359e+00,  1.4753e+00, -2.6225e-01, -1.3580e+00,\n",
              "          1.0898e+00, -5.0837e-02,  5.2203e-02, -9.9044e-01,  6.7885e-01,\n",
              "          1.0061e+00, -7.1469e-01,  1.2832e+00,  2.1572e-01, -4.3626e-01,\n",
              "         -1.0389e+00, -5.3543e-01,  5.3068e-01, -1.7765e+00, -2.7684e-01,\n",
              "          2.9175e-01, -1.6760e-01,  7.0996e-01,  1.8965e+00, -1.0857e-01,\n",
              "          7.1688e-01,  9.2957e-01, -5.1933e-01,  4.6385e-01,  1.3670e+00,\n",
              "         -3.4656e-01]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "outputs[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSrt1UEZ3d3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b778dd25-5721-441b-99ac-540103fc8041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample probabilities:\n",
            " tensor([[0.0014, 0.0019, 0.0023, 0.0013, 0.0017, 0.0009, 0.0026, 0.0015, 0.0024,\n",
            "         0.0019, 0.0008, 0.0017, 0.0009, 0.0024, 0.0011, 0.0004, 0.0027, 0.0014,\n",
            "         0.0091, 0.0015, 0.0030, 0.0008, 0.0035, 0.0008, 0.0024, 0.0015, 0.0011,\n",
            "         0.0018, 0.0017, 0.0037, 0.0025, 0.0015, 0.0022, 0.0012, 0.0016, 0.0011,\n",
            "         0.0035, 0.0011, 0.0081, 0.0085, 0.0050, 0.0031, 0.0010, 0.0023, 0.0032,\n",
            "         0.0017, 0.0020, 0.0014, 0.0018, 0.0042, 0.0013, 0.0022, 0.0005, 0.0013,\n",
            "         0.0031, 0.0021, 0.0012, 0.0015, 0.0017, 0.0029, 0.0005, 0.0009, 0.0056,\n",
            "         0.0020, 0.0017, 0.0036, 0.0022, 0.0021, 0.0012, 0.0024, 0.0061, 0.0010,\n",
            "         0.0016, 0.0044, 0.0003, 0.0012, 0.0020, 0.0006, 0.0006, 0.0009, 0.0014,\n",
            "         0.0020, 0.0024, 0.0052, 0.0035, 0.0011, 0.0039, 0.0043, 0.0035, 0.0005,\n",
            "         0.0013, 0.0009, 0.0041, 0.0032, 0.0020, 0.0012, 0.0042, 0.0027, 0.0035,\n",
            "         0.0017, 0.0017, 0.0047, 0.0029, 0.0011, 0.0047, 0.0016, 0.0009, 0.0014,\n",
            "         0.0009, 0.0021, 0.0066, 0.0043, 0.0025, 0.0011, 0.0022, 0.0074, 0.0012,\n",
            "         0.0011, 0.0017, 0.0019, 0.0054, 0.0022, 0.0015, 0.0035, 0.0025, 0.0041,\n",
            "         0.0016, 0.0007, 0.0019, 0.0019, 0.0006, 0.0022, 0.0053, 0.0012, 0.0015,\n",
            "         0.0025, 0.0013, 0.0052, 0.0013, 0.0020, 0.0014, 0.0011, 0.0059, 0.0012,\n",
            "         0.0019, 0.0018, 0.0012, 0.0010, 0.0058, 0.0049, 0.0010, 0.0009, 0.0027,\n",
            "         0.0010, 0.0056, 0.0020, 0.0031, 0.0012, 0.0008, 0.0008, 0.0021, 0.0040,\n",
            "         0.0022, 0.0009, 0.0033, 0.0019, 0.0030, 0.0009, 0.0018, 0.0029, 0.0027,\n",
            "         0.0021, 0.0018, 0.0026, 0.0013, 0.0007, 0.0021, 0.0005, 0.0038, 0.0038,\n",
            "         0.0068, 0.0016, 0.0018, 0.0007, 0.0018, 0.0020, 0.0135, 0.0021, 0.0012,\n",
            "         0.0034, 0.0010, 0.0019, 0.0028, 0.0031, 0.0032, 0.0017, 0.0015, 0.0006,\n",
            "         0.0010, 0.0005, 0.0014, 0.0017, 0.0034, 0.0008, 0.0009, 0.0010, 0.0010,\n",
            "         0.0064, 0.0004, 0.0057, 0.0024, 0.0011, 0.0026, 0.0020, 0.0048, 0.0016,\n",
            "         0.0019, 0.0020, 0.0020, 0.0048, 0.0025, 0.0026, 0.0010, 0.0035, 0.0023,\n",
            "         0.0024, 0.0052, 0.0019, 0.0028, 0.0009, 0.0020, 0.0012, 0.0026, 0.0026,\n",
            "         0.0021, 0.0009, 0.0007, 0.0015, 0.0012, 0.0011, 0.0017, 0.0014, 0.0018,\n",
            "         0.0025, 0.0033, 0.0078, 0.0007, 0.0028, 0.0012, 0.0028, 0.0018, 0.0017,\n",
            "         0.0008, 0.0010, 0.0014, 0.0029, 0.0011, 0.0008, 0.0023, 0.0009, 0.0108,\n",
            "         0.0038, 0.0011, 0.0018, 0.0016, 0.0012, 0.0022, 0.0009, 0.0012, 0.0022,\n",
            "         0.0009, 0.0059, 0.0025, 0.0021, 0.0037, 0.0016, 0.0029, 0.0020, 0.0018,\n",
            "         0.0019, 0.0010, 0.0010, 0.0029, 0.0027, 0.0035, 0.0016, 0.0014, 0.0013,\n",
            "         0.0022, 0.0016, 0.0010, 0.0013, 0.0070, 0.0011, 0.0086, 0.0021, 0.0018,\n",
            "         0.0015, 0.0011, 0.0018, 0.0014, 0.0021, 0.0027, 0.0013, 0.0018, 0.0017,\n",
            "         0.0040, 0.0018, 0.0014, 0.0017, 0.0008, 0.0008, 0.0012, 0.0005, 0.0009,\n",
            "         0.0026, 0.0035, 0.0019, 0.0057, 0.0026, 0.0030, 0.0014, 0.0033, 0.0005,\n",
            "         0.0014, 0.0004, 0.0031, 0.0044, 0.0018, 0.0024, 0.0020, 0.0019, 0.0022,\n",
            "         0.0025, 0.0015, 0.0017, 0.0043, 0.0018, 0.0015, 0.0013, 0.0025, 0.0039,\n",
            "         0.0014, 0.0018, 0.0024, 0.0126, 0.0009, 0.0016, 0.0019, 0.0036, 0.0013,\n",
            "         0.0026, 0.0016, 0.0015, 0.0020, 0.0013, 0.0026, 0.0029, 0.0025, 0.0029,\n",
            "         0.0013, 0.0021, 0.0018, 0.0029, 0.0023, 0.0014, 0.0010, 0.0006, 0.0037,\n",
            "         0.0022, 0.0031, 0.0020, 0.0012, 0.0007, 0.0038, 0.0009, 0.0081, 0.0022,\n",
            "         0.0039, 0.0026, 0.0016, 0.0014, 0.0062, 0.0013, 0.0006, 0.0011, 0.0007,\n",
            "         0.0015, 0.0018, 0.0004, 0.0020, 0.0016, 0.0009, 0.0011, 0.0021, 0.0010,\n",
            "         0.0013, 0.0022, 0.0025, 0.0012, 0.0005, 0.0026, 0.0025, 0.0042, 0.0079,\n",
            "         0.0012, 0.0034, 0.0010, 0.0041, 0.0004, 0.0012, 0.0011, 0.0102, 0.0012,\n",
            "         0.0029, 0.0047, 0.0014, 0.0055, 0.0028, 0.0052, 0.0040, 0.0038, 0.0013,\n",
            "         0.0007, 0.0023, 0.0016, 0.0028, 0.0019, 0.0004, 0.0013, 0.0006],\n",
            "        [0.0021, 0.0008, 0.0008, 0.0007, 0.0009, 0.0062, 0.0035, 0.0022, 0.0026,\n",
            "         0.0007, 0.0034, 0.0024, 0.0011, 0.0023, 0.0042, 0.0013, 0.0019, 0.0017,\n",
            "         0.0007, 0.0015, 0.0009, 0.0102, 0.0032, 0.0053, 0.0015, 0.0029, 0.0005,\n",
            "         0.0077, 0.0014, 0.0008, 0.0003, 0.0010, 0.0023, 0.0018, 0.0007, 0.0022,\n",
            "         0.0004, 0.0035, 0.0004, 0.0005, 0.0014, 0.0017, 0.0040, 0.0018, 0.0009,\n",
            "         0.0052, 0.0011, 0.0006, 0.0004, 0.0041, 0.0005, 0.0010, 0.0010, 0.0026,\n",
            "         0.0054, 0.0024, 0.0009, 0.0009, 0.0011, 0.0006, 0.0066, 0.0031, 0.0016,\n",
            "         0.0008, 0.0080, 0.0013, 0.0002, 0.0006, 0.0036, 0.0024, 0.0052, 0.0015,\n",
            "         0.0022, 0.0005, 0.0022, 0.0012, 0.0060, 0.0011, 0.0006, 0.0035, 0.0033,\n",
            "         0.0015, 0.0021, 0.0005, 0.0001, 0.0008, 0.0007, 0.0008, 0.0007, 0.0022,\n",
            "         0.0012, 0.0022, 0.0029, 0.0040, 0.0008, 0.0040, 0.0004, 0.0019, 0.0007,\n",
            "         0.0020, 0.0039, 0.0013, 0.0055, 0.0019, 0.0015, 0.0003, 0.0049, 0.0011,\n",
            "         0.0065, 0.0009, 0.0005, 0.0013, 0.0056, 0.0007, 0.0009, 0.0004, 0.0007,\n",
            "         0.0005, 0.0059, 0.0047, 0.0017, 0.0015, 0.0018, 0.0028, 0.0005, 0.0013,\n",
            "         0.0037, 0.0006, 0.0010, 0.0027, 0.0009, 0.0027, 0.0011, 0.0013, 0.0028,\n",
            "         0.0016, 0.0041, 0.0005, 0.0026, 0.0011, 0.0018, 0.0033, 0.0009, 0.0042,\n",
            "         0.0047, 0.0011, 0.0024, 0.0042, 0.0013, 0.0012, 0.0007, 0.0032, 0.0022,\n",
            "         0.0005, 0.0028, 0.0036, 0.0007, 0.0141, 0.0105, 0.0032, 0.0019, 0.0005,\n",
            "         0.0021, 0.0067, 0.0016, 0.0009, 0.0009, 0.0016, 0.0016, 0.0021, 0.0007,\n",
            "         0.0039, 0.0044, 0.0018, 0.0011, 0.0030, 0.0006, 0.0014, 0.0007, 0.0022,\n",
            "         0.0062, 0.0046, 0.0010, 0.0068, 0.0022, 0.0026, 0.0010, 0.0025, 0.0012,\n",
            "         0.0024, 0.0027, 0.0006, 0.0003, 0.0011, 0.0081, 0.0017, 0.0017, 0.0025,\n",
            "         0.0049, 0.0008, 0.0072, 0.0020, 0.0014, 0.0012, 0.0024, 0.0012, 0.0005,\n",
            "         0.0001, 0.0032, 0.0007, 0.0057, 0.0019, 0.0010, 0.0017, 0.0036, 0.0018,\n",
            "         0.0029, 0.0059, 0.0008, 0.0016, 0.0016, 0.0005, 0.0009, 0.0006, 0.0056,\n",
            "         0.0008, 0.0007, 0.0016, 0.0009, 0.0005, 0.0012, 0.0009, 0.0028, 0.0012,\n",
            "         0.0008, 0.0020, 0.0015, 0.0005, 0.0016, 0.0016, 0.0007, 0.0032, 0.0030,\n",
            "         0.0022, 0.0020, 0.0007, 0.0024, 0.0013, 0.0009, 0.0031, 0.0011, 0.0017,\n",
            "         0.0021, 0.0028, 0.0021, 0.0012, 0.0021, 0.0017, 0.0009, 0.0006, 0.0010,\n",
            "         0.0033, 0.0010, 0.0008, 0.0059, 0.0018, 0.0017, 0.0010, 0.0006, 0.0019,\n",
            "         0.0026, 0.0010, 0.0003, 0.0004, 0.0010, 0.0004, 0.0006, 0.0015, 0.0076,\n",
            "         0.0052, 0.0050, 0.0043, 0.0033, 0.0010, 0.0005, 0.0042, 0.0004, 0.0073,\n",
            "         0.0014, 0.0040, 0.0027, 0.0010, 0.0019, 0.0094, 0.0037, 0.0006, 0.0010,\n",
            "         0.0049, 0.0008, 0.0022, 0.0025, 0.0090, 0.0013, 0.0021, 0.0004, 0.0003,\n",
            "         0.0017, 0.0020, 0.0007, 0.0006, 0.0035, 0.0015, 0.0005, 0.0012, 0.0024,\n",
            "         0.0015, 0.0054, 0.0012, 0.0015, 0.0040, 0.0004, 0.0008, 0.0018, 0.0014,\n",
            "         0.0075, 0.0011, 0.0010, 0.0060, 0.0011, 0.0042, 0.0036, 0.0028, 0.0014,\n",
            "         0.0060, 0.0049, 0.0019, 0.0016, 0.0049, 0.0054, 0.0011, 0.0040, 0.0019,\n",
            "         0.0036, 0.0038, 0.0009, 0.0012, 0.0011, 0.0050, 0.0009, 0.0033, 0.0009,\n",
            "         0.0022, 0.0007, 0.0028, 0.0009, 0.0053, 0.0024, 0.0009, 0.0022, 0.0014,\n",
            "         0.0006, 0.0012, 0.0014, 0.0013, 0.0019, 0.0026, 0.0010, 0.0012, 0.0021,\n",
            "         0.0017, 0.0028, 0.0010, 0.0003, 0.0010, 0.0026, 0.0015, 0.0014, 0.0013,\n",
            "         0.0016, 0.0003, 0.0033, 0.0010, 0.0004, 0.0090, 0.0005, 0.0012, 0.0019,\n",
            "         0.0013, 0.0011, 0.0041, 0.0022, 0.0052, 0.0026, 0.0251, 0.0006, 0.0046,\n",
            "         0.0012, 0.0041, 0.0012, 0.0005, 0.0010, 0.0047, 0.0072, 0.0013, 0.0004,\n",
            "         0.0049, 0.0016, 0.0017, 0.0006, 0.0033, 0.0045, 0.0008, 0.0060, 0.0021,\n",
            "         0.0011, 0.0006, 0.0010, 0.0028, 0.0003, 0.0013, 0.0022, 0.0014, 0.0034,\n",
            "         0.0110, 0.0015, 0.0034, 0.0042, 0.0010, 0.0026, 0.0065, 0.0012]])\n",
            "Sum:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Apply softmax for each output row\n",
        "probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "# Look at sample probabilities\n",
        "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
        "\n",
        "# Add up the probabilities of an output row\n",
        "print(\"Sum: \", torch.sum(probs[0]).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can determine the predicted label for each image by simply choosing the index of the element with the highest probability in each output row. We can do this using `torch.max`, which returns each row's largest element and the corresponding index."
      ],
      "metadata": {
        "id": "Vn00KqMVV5HE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWwFI8Wj3m0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14152201-4ba2-4d5d-e158-44b1755418be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([186, 393, 393, 287,  52, 267, 296,  54, 276, 386, 423, 340, 395, 389,\n",
            "        216, 393, 208, 356,  18, 351, 221, 190,  95, 389, 157,  43, 200, 331,\n",
            "        173, 227, 121, 108])\n",
            "tensor([0.0135, 0.0251, 0.0188, 0.0180, 0.0088, 0.0173, 0.0169, 0.0142, 0.0106,\n",
            "        0.0110, 0.0186, 0.0190, 0.0168, 0.0193, 0.0094, 0.0196, 0.0109, 0.0124,\n",
            "        0.0153, 0.0147, 0.0147, 0.0127, 0.0106, 0.0163, 0.0179, 0.0125, 0.0172,\n",
            "        0.0138, 0.0247, 0.0144, 0.0097, 0.0273], grad_fn=<MaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "print(preds)\n",
        "print(max_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE9ZQ-3p3rP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b258c04-4a6e-42eb-ff96-3c289c9dcb86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([137, 361, 200, 282,  70, 373, 396, 309,  59,  31,  54, 407, 220,  76,\n",
              "         77, 135, 198, 322, 412, 226, 260, 127,  26, 313, 158, 152, 404, 420,\n",
              "          4,  18, 184, 193])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz_EElzH3w-Z"
      },
      "source": [
        "**Evaluation Metric and Loss Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as with linear regression, we need a way to evaluate how well our model is performing. A natural way to do this would be to find the percentage of labels that were predicted correctly, i.e,. the **accuracy** of the predictions. "
      ],
      "metadata": {
        "id": "zbV6-xhRV9n3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G-oXIPR3tWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dbac406-56fe-463e-e63c-fe9a4ba2c4af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.6649e-01, -5.9991e-03,  2.0771e-01, -3.7419e-01, -1.0610e-01,\n",
              "         -7.0066e-01,  3.1469e-01, -1.9118e-01,  2.4316e-01,  2.5109e-02,\n",
              "         -7.9865e-01, -7.0293e-02, -7.8035e-01,  2.6730e-01, -5.4691e-01,\n",
              "         -1.6654e+00,  3.8568e-01, -3.0592e-01,  1.5885e+00, -2.1776e-01,\n",
              "          4.8043e-01, -8.9693e-01,  6.2812e-01, -8.0487e-01,  2.4533e-01,\n",
              "         -2.2773e-01, -4.9023e-01, -5.7397e-02, -7.1978e-02,  6.8417e-01,\n",
              "          3.0468e-01, -2.2018e-01,  1.7091e-01, -4.3361e-01, -1.3614e-01,\n",
              "         -5.2088e-01,  6.4118e-01, -5.3125e-01,  1.4665e+00,  1.5174e+00,\n",
              "          9.7967e-01,  5.0859e-01, -6.6972e-01,  1.9123e-01,  5.3745e-01,\n",
              "         -1.2128e-01,  7.6711e-02, -3.0778e-01, -4.6009e-02,  8.0425e-01,\n",
              "         -3.7026e-01,  1.5258e-01, -1.3606e+00, -3.5183e-01,  5.1647e-01,\n",
              "          1.1943e-01, -4.1414e-01, -1.8972e-01, -7.1182e-02,  4.3164e-01,\n",
              "         -1.3351e+00, -6.8693e-01,  1.0996e+00,  6.7960e-02, -9.8906e-02,\n",
              "          6.6148e-01,  1.6139e-01,  1.0055e-01, -4.1256e-01,  2.3737e-01,\n",
              "          1.1871e+00, -6.2013e-01, -1.7179e-01,  8.6545e-01, -1.9558e+00,\n",
              "         -4.5724e-01,  7.0063e-02, -1.1102e+00, -1.0603e+00, -7.0850e-01,\n",
              "         -3.2149e-01,  6.1530e-02,  2.6978e-01,  1.0194e+00,  6.3125e-01,\n",
              "         -5.0130e-01,  7.3629e-01,  8.3672e-01,  6.4149e-01, -1.3437e+00,\n",
              "         -3.4268e-01, -7.7218e-01,  7.9189e-01,  5.3550e-01,  6.0389e-02,\n",
              "         -4.3916e-01,  8.1574e-01,  3.5993e-01,  6.3937e-01, -1.0572e-01,\n",
              "         -1.0363e-01,  9.1991e-01,  4.5471e-01, -5.0576e-01,  9.2609e-01,\n",
              "         -1.6616e-01, -7.8292e-01, -3.1712e-01, -7.2174e-01,  9.4854e-02,\n",
              "          1.2569e+00,  8.3142e-01,  3.1027e-01, -5.5438e-01,  1.6920e-01,\n",
              "          1.3815e+00, -4.0292e-01, -5.4076e-01, -1.1493e-01,  3.8152e-02,\n",
              "          1.0589e+00,  1.4502e-01, -2.2963e-01,  6.4277e-01,  2.8132e-01,\n",
              "          7.9862e-01, -1.3653e-01, -9.6251e-01,  6.8168e-03,  2.7801e-02,\n",
              "         -1.1833e+00,  1.4858e-01,  1.0511e+00, -4.3835e-01, -2.3269e-01,\n",
              "          2.9281e-01, -3.9291e-01,  1.0341e+00, -3.5507e-01,  4.7759e-02,\n",
              "         -2.9888e-01, -5.4615e-01,  1.1440e+00, -4.4812e-01,  3.6858e-02,\n",
              "         -3.2022e-02, -4.7930e-01, -6.7097e-01,  1.1335e+00,  9.6531e-01,\n",
              "         -6.6374e-01, -7.5585e-01,  3.6964e-01, -6.2033e-01,  1.1047e+00,\n",
              "          9.3066e-02,  5.0421e-01, -4.1701e-01, -9.0490e-01, -8.4097e-01,\n",
              "          1.1018e-01,  7.5689e-01,  1.7275e-01, -7.6223e-01,  5.7520e-01,\n",
              "          4.2371e-02,  4.7680e-01, -7.0713e-01, -3.8828e-02,  4.2641e-01,\n",
              "          3.5636e-01,  1.1014e-01, -3.6608e-02,  3.1858e-01, -3.8282e-01,\n",
              "         -9.9895e-01,  1.2076e-01, -1.3374e+00,  7.0342e-01,  7.1355e-01,\n",
              "          1.2920e+00, -1.7174e-01, -4.9302e-02, -9.7964e-01, -1.2666e-02,\n",
              "          5.7145e-02,  1.9793e+00,  1.0565e-01, -4.7164e-01,  6.1177e-01,\n",
              "         -6.7420e-01,  2.4594e-02,  4.0145e-01,  5.0064e-01,  5.3819e-01,\n",
              "         -7.8765e-02, -2.2561e-01, -1.0589e+00, -6.7161e-01, -1.3370e+00,\n",
              "         -2.8852e-01, -1.0146e-01,  6.1145e-01, -8.7811e-01, -6.9947e-01,\n",
              "         -6.3487e-01, -5.9324e-01,  1.2261e+00, -1.5011e+00,  1.1196e+00,\n",
              "          2.3160e-01, -5.6532e-01,  3.3098e-01,  5.3730e-02,  9.3415e-01,\n",
              "         -1.6406e-01,  9.1403e-03,  4.8434e-02,  6.8331e-02,  9.4644e-01,\n",
              "          2.7792e-01,  3.1853e-01, -6.3992e-01,  6.2178e-01,  2.1019e-01,\n",
              "          2.3101e-01,  1.0277e+00,  1.2515e-02,  4.2150e-01, -7.2762e-01,\n",
              "          5.8563e-02, -4.5661e-01,  3.2690e-01,  3.1199e-01,  1.0504e-01,\n",
              "         -7.6822e-01, -1.0306e+00, -1.9351e-01, -4.2837e-01, -5.1585e-01,\n",
              "         -8.6734e-02, -2.5916e-01, -4.4342e-02,  3.0075e-01,  5.6710e-01,\n",
              "          1.4316e+00, -1.0461e+00,  4.0321e-01, -4.2482e-01,  3.9069e-01,\n",
              "         -5.3215e-02, -7.6812e-02, -8.1557e-01, -6.1453e-01, -3.0923e-01,\n",
              "          4.3192e-01, -5.1136e-01, -8.7456e-01,  1.9333e-01, -7.0170e-01,\n",
              "          1.7531e+00,  7.1061e-01, -5.3399e-01, -1.1258e-02, -1.4031e-01,\n",
              "         -4.0806e-01,  1.7729e-01, -7.3940e-01, -4.3006e-01,  1.4778e-01,\n",
              "         -7.6466e-01,  1.1463e+00,  2.9309e-01,  1.3241e-01,  6.7125e-01,\n",
              "         -1.8531e-01,  4.2409e-01,  9.2425e-02, -2.2919e-02,  3.3750e-02,\n",
              "         -6.0580e-01, -6.2436e-01,  4.3790e-01,  3.6833e-01,  6.2528e-01,\n",
              "         -1.4303e-01, -2.6318e-01, -3.8953e-01,  1.5864e-01, -1.3642e-01,\n",
              "         -5.9182e-01, -3.3539e-01,  1.3203e+00, -5.0096e-01,  1.5245e+00,\n",
              "          1.0805e-01, -1.2631e-02, -1.8749e-01, -5.2178e-01, -3.5947e-02,\n",
              "         -3.1489e-01,  9.6305e-02,  3.8667e-01, -3.5292e-01, -5.0717e-02,\n",
              "         -1.1000e-01,  7.6344e-01, -1.3826e-02, -2.7162e-01, -1.1048e-01,\n",
              "         -8.1743e-01, -9.0955e-01, -4.0890e-01, -1.3077e+00, -6.8516e-01,\n",
              "          3.4415e-01,  6.3110e-01,  3.9104e-02,  1.1239e+00,  3.4018e-01,\n",
              "          4.5994e-01, -2.9974e-01,  5.7424e-01, -1.2917e+00, -2.7770e-01,\n",
              "         -1.4407e+00,  4.9791e-01,  8.6339e-01, -2.2258e-02,  2.3699e-01,\n",
              "          7.2081e-02,  1.2197e-02,  1.7812e-01,  2.8017e-01, -2.0013e-01,\n",
              "         -1.0811e-01,  8.3850e-01, -5.0951e-02, -2.1956e-01, -3.4104e-01,\n",
              "          2.8042e-01,  7.4830e-01, -3.0467e-01, -2.1437e-02,  2.4523e-01,\n",
              "          1.9107e+00, -7.2158e-01, -1.7347e-01,  2.9271e-02,  6.6414e-01,\n",
              "         -3.7068e-01,  3.4662e-01, -1.3598e-01, -2.3390e-01,  7.1911e-02,\n",
              "         -3.4857e-01,  3.2390e-01,  4.5053e-01,  2.7539e-01,  4.2396e-01,\n",
              "         -3.3908e-01,  1.2414e-01, -4.7158e-02,  4.2349e-01,  2.0388e-01,\n",
              "         -2.8991e-01, -6.0664e-01, -1.0557e+00,  6.8169e-01,  1.4372e-01,\n",
              "          4.9542e-01,  5.2802e-02, -4.3358e-01, -1.0154e+00,  7.0501e-01,\n",
              "         -7.1561e-01,  1.4681e+00,  1.7612e-01,  7.3543e-01,  3.1453e-01,\n",
              "         -1.7526e-01, -2.6634e-01,  1.2080e+00, -3.5939e-01, -1.0774e+00,\n",
              "         -5.3320e-01, -9.6798e-01, -2.1823e-01, -2.2483e-02, -1.6394e+00,\n",
              "          5.4933e-02, -1.5625e-01, -7.1890e-01, -5.1505e-01,  9.7430e-02,\n",
              "         -5.8235e-01, -3.7881e-01,  1.5534e-01,  2.9725e-01, -4.5980e-01,\n",
              "         -1.4106e+00,  3.5009e-01,  2.9507e-01,  8.0823e-01,  1.4402e+00,\n",
              "         -4.7553e-01,  5.9858e-01, -6.6094e-01,  7.9237e-01, -1.5184e+00,\n",
              "         -4.7851e-01, -5.7514e-01,  1.7030e+00, -4.1418e-01,  4.4532e-01,\n",
              "          9.1360e-01, -2.6257e-01,  1.0812e+00,  3.8779e-01,  1.0327e+00,\n",
              "          7.5560e-01,  7.1686e-01, -3.6724e-01, -9.5899e-01,  1.9424e-01,\n",
              "         -1.8445e-01,  3.9652e-01, -9.1429e-04, -1.4303e+00, -3.2508e-01,\n",
              "         -1.0903e+00],\n",
              "        [ 2.5603e-01, -7.1324e-01, -7.1967e-01, -8.4554e-01, -6.4591e-01,\n",
              "          1.3197e+00,  7.6177e-01,  2.6632e-01,  4.3446e-01, -7.9389e-01,\n",
              "          7.2788e-01,  3.5445e-01, -3.7735e-01,  3.3504e-01,  9.3321e-01,\n",
              "         -2.2945e-01,  1.6167e-01,  1.3383e-02, -9.1503e-01, -1.1522e-01,\n",
              "         -6.1435e-01,  1.8199e+00,  6.6777e-01,  1.1592e+00, -1.0737e-01,\n",
              "          5.4494e-01, -1.2062e+00,  1.5319e+00, -1.4490e-01, -7.2997e-01,\n",
              "         -1.5604e+00, -5.0435e-01,  3.2512e-01,  6.5534e-02, -9.1867e-01,\n",
              "          2.7986e-01, -1.5058e+00,  7.3777e-01, -1.3063e+00, -1.1860e+00,\n",
              "         -1.3438e-01,  1.3672e-03,  8.8443e-01,  7.1328e-02, -6.4961e-01,\n",
              "          1.1508e+00, -3.7876e-01, -1.0950e+00, -1.4732e+00,  9.0619e-01,\n",
              "         -1.2413e+00, -5.0067e-01, -5.2251e-01,  4.3243e-01,  1.1773e+00,\n",
              "          3.7435e-01, -5.6747e-01, -5.6075e-01, -4.4487e-01, -9.4211e-01,\n",
              "          1.3886e+00,  6.1655e-01, -2.9219e-02, -6.9411e-01,  1.5772e+00,\n",
              "         -2.4705e-01, -2.0996e+00, -9.5644e-01,  7.8392e-01,  3.7212e-01,\n",
              "          1.1443e+00, -9.1701e-02,  3.0563e-01, -1.1534e+00,  3.0504e-01,\n",
              "         -3.3520e-01,  1.2870e+00, -3.9834e-01, -1.0124e+00,  7.4552e-01,\n",
              "          7.0079e-01, -1.3062e-01,  2.6059e-01, -1.1108e+00, -2.4825e+00,\n",
              "         -6.9267e-01, -8.9930e-01, -7.5877e-01, -8.0919e-01,  3.0447e-01,\n",
              "         -3.3349e-01,  2.6946e-01,  5.7015e-01,  8.6994e-01, -7.2145e-01,\n",
              "          8.7883e-01, -1.4392e+00,  1.5426e-01, -7.9254e-01,  1.9998e-01,\n",
              "          8.5024e-01, -2.6332e-01,  1.2069e+00,  1.3135e-01, -8.3295e-02,\n",
              "         -1.6019e+00,  1.0831e+00, -4.1808e-01,  1.3611e+00, -5.6642e-01,\n",
              "         -1.1248e+00, -2.6765e-01,  1.2137e+00, -8.9705e-01, -6.1245e-01,\n",
              "         -1.3998e+00, -8.7452e-01, -1.2425e+00,  1.2775e+00,  1.0462e+00,\n",
              "          2.2433e-02, -1.3156e-01,  9.4560e-02,  5.2234e-01, -1.1868e+00,\n",
              "         -2.1071e-01,  7.9099e-01, -9.5959e-01, -5.5506e-01,  4.7540e-01,\n",
              "         -5.6722e-01,  4.8215e-01, -3.9175e-01, -2.5481e-01,  5.2872e-01,\n",
              "         -6.2592e-02,  9.0851e-01, -1.1996e+00,  4.4784e-01, -3.8398e-01,\n",
              "          7.3516e-02,  6.9608e-01, -6.5399e-01,  9.1876e-01,  1.0456e+00,\n",
              "         -4.1964e-01,  3.6172e-01,  9.2633e-01, -2.7194e-01, -3.4964e-01,\n",
              "         -8.7674e-01,  6.4280e-01,  2.9639e-01, -1.2666e+00,  5.2152e-01,\n",
              "          7.8229e-01, -8.6223e-01,  2.1443e+00,  1.8443e+00,  6.5370e-01,\n",
              "          1.4375e-01, -1.1078e+00,  2.5774e-01,  1.4006e+00, -6.0557e-02,\n",
              "         -6.0645e-01, -5.6105e-01, -4.3706e-02, -6.1488e-02,  2.2302e-01,\n",
              "         -8.1128e-01,  8.4881e-01,  9.8719e-01,  6.6740e-02, -4.3942e-01,\n",
              "          6.0916e-01, -1.0401e+00, -1.6904e-01, -7.9668e-01,  2.8153e-01,\n",
              "          1.3239e+00,  1.0178e+00, -5.1435e-01,  1.4125e+00,  2.9240e-01,\n",
              "          4.4670e-01, -5.4484e-01,  4.2875e-01, -3.3974e-01,  3.8248e-01,\n",
              "          4.9644e-01, -1.0042e+00, -1.7849e+00, -3.7511e-01,  1.5846e+00,\n",
              "         -3.7804e-03,  4.9275e-02,  4.0950e-01,  1.0768e+00, -7.0005e-01,\n",
              "          1.4701e+00,  1.7109e-01, -1.9529e-01, -3.4588e-01,  3.8041e-01,\n",
              "         -3.3773e-01, -1.1314e+00, -2.4053e+00,  6.6326e-01, -8.9633e-01,\n",
              "          1.2384e+00,  1.5494e-01, -5.1893e-01,  1.8940e-02,  7.7267e-01,\n",
              "          1.0871e-01,  5.6111e-01,  1.2630e+00, -7.6388e-01, -5.8697e-03,\n",
              "         -3.6886e-02, -1.3012e+00, -6.2903e-01, -9.4983e-01,  1.2121e+00,\n",
              "         -7.7395e-01, -7.9955e-01, -2.4683e-02, -6.2256e-01, -1.2200e+00,\n",
              "         -3.2801e-01, -5.5645e-01,  5.4229e-01, -2.9409e-01, -7.3464e-01,\n",
              "          1.9392e-01, -1.0395e-01, -1.1647e+00, -9.9294e-03, -2.8010e-02,\n",
              "         -8.5605e-01,  6.6953e-01,  5.9072e-01,  2.7137e-01,  1.7121e-01,\n",
              "         -9.0983e-01,  3.6251e-01, -2.6866e-01, -6.6471e-01,  6.2880e-01,\n",
              "         -4.2859e-01,  3.2046e-02,  2.2932e-01,  5.2863e-01,  2.1417e-01,\n",
              "         -2.8897e-01,  2.4733e-01,  4.8851e-02, -6.4772e-01, -1.0839e+00,\n",
              "         -5.1585e-01,  6.7840e-01, -5.3798e-01, -6.7101e-01,  1.2739e+00,\n",
              "          6.9670e-02,  1.0125e-02, -5.2771e-01, -9.6526e-01,  1.2259e-01,\n",
              "          4.3712e-01, -5.5094e-01, -1.6795e+00, -1.5128e+00, -4.8495e-01,\n",
              "         -1.5389e+00, -9.9888e-01, -9.1453e-02,  1.5259e+00,  1.1483e+00,\n",
              "          1.1097e+00,  9.5077e-01,  6.7717e-01, -5.1033e-01, -1.2239e+00,\n",
              "          9.2210e-01, -1.4161e+00,  1.4811e+00, -1.7611e-01,  8.9202e-01,\n",
              "          4.7485e-01, -5.2845e-01,  1.2280e-01,  1.7376e+00,  8.1058e-01,\n",
              "         -9.6744e-01, -5.3657e-01,  1.0842e+00, -7.4364e-01,  2.8135e-01,\n",
              "          4.3070e-01,  1.6902e+00, -2.6781e-01,  2.3456e-01, -1.4908e+00,\n",
              "         -1.6727e+00,  9.7076e-03,  1.8494e-01, -9.2255e-01, -1.0548e+00,\n",
              "          7.3956e-01, -7.6959e-02, -1.1083e+00, -3.0063e-01,  3.5735e-01,\n",
              "         -7.4811e-02,  1.1821e+00, -3.0897e-01, -1.2252e-01,  8.8708e-01,\n",
              "         -1.3590e+00, -6.8218e-01,  1.0654e-01, -1.8007e-01,  1.5150e+00,\n",
              "         -4.1820e-01, -5.5596e-01,  1.2887e+00, -4.4786e-01,  9.3518e-01,\n",
              "          7.8680e-01,  5.3313e-01, -1.5024e-01,  1.2865e+00,  1.0876e+00,\n",
              "          1.3063e-01, -3.0401e-02,  1.0846e+00,  1.1851e+00, -4.2530e-01,\n",
              "          8.8967e-01,  1.1379e-01,  7.7512e-01,  8.1924e-01, -6.4804e-01,\n",
              "         -3.2721e-01, -3.8847e-01,  1.1013e+00, -6.1011e-01,  6.8301e-01,\n",
              "         -5.6665e-01,  2.6738e-01, -9.1218e-01,  5.0870e-01, -5.8891e-01,\n",
              "          1.1564e+00,  3.8836e-01, -5.8187e-01,  2.7406e-01, -1.8718e-01,\n",
              "         -1.0160e+00, -2.8389e-01, -1.9977e-01, -2.5817e-01,  1.6194e-01,\n",
              "          4.4441e-01, -4.8075e-01, -3.5324e-01,  2.3039e-01,  2.8011e-02,\n",
              "          5.2083e-01, -5.3109e-01, -1.6585e+00, -5.4438e-01,  4.3566e-01,\n",
              "         -1.2875e-01, -2.0025e-01, -2.7136e-01, -1.1682e-02, -1.7767e+00,\n",
              "          6.8691e-01, -4.9904e-01, -1.4772e+00,  1.6936e+00, -1.1119e+00,\n",
              "         -3.3611e-01,  1.6016e-01, -2.4432e-01, -3.9504e-01,  8.9875e-01,\n",
              "          2.9992e-01,  1.1382e+00,  4.4606e-01,  2.7185e+00, -1.0644e+00,\n",
              "          1.0307e+00, -3.1823e-01,  9.0443e-01, -3.1055e-01, -1.2264e+00,\n",
              "         -4.9938e-01,  1.0359e+00,  1.4753e+00, -2.6225e-01, -1.3580e+00,\n",
              "          1.0898e+00, -5.0837e-02,  5.2203e-02, -9.9044e-01,  6.7885e-01,\n",
              "          1.0061e+00, -7.1469e-01,  1.2832e+00,  2.1572e-01, -4.3626e-01,\n",
              "         -1.0389e+00, -5.3543e-01,  5.3068e-01, -1.7765e+00, -2.7684e-01,\n",
              "          2.9175e-01, -1.6760e-01,  7.0996e-01,  1.8965e+00, -1.0857e-01,\n",
              "          7.1688e-01,  9.2957e-01, -5.1933e-01,  4.6385e-01,  1.3670e+00,\n",
              "         -3.4656e-01]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "outputs[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qshJslBf31A9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc88a6b2-7184-4ec3-a4c8-2651ba760765"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "torch.sum(preds == labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwykObKj35v2"
      },
      "outputs": [],
      "source": [
        "def accuracy(l1, l2):\n",
        "    return torch.sum(l1, l2).item() / len(l1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnxNHyuD40xL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d41d390-4108-4267-c5b4-d6ee9857fc47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.) \n",
            " 0.0\n"
          ]
        }
      ],
      "source": [
        "accuracy1 = torch.sum(labels == preds) / len(labels)\n",
        "accuracy2 = torch.sum(labels == preds).item() / len(labels)\n",
        "print(accuracy1, \"\\n\", accuracy2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y80pLaT5AIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5701bad-fc05-41cf-e4c1-8e9d6eb97394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.6649e-01, -5.9991e-03,  2.0771e-01, -3.7419e-01, -1.0610e-01,\n",
            "         -7.0066e-01,  3.1469e-01, -1.9118e-01,  2.4316e-01,  2.5109e-02,\n",
            "         -7.9865e-01, -7.0293e-02, -7.8035e-01,  2.6730e-01, -5.4691e-01,\n",
            "         -1.6654e+00,  3.8568e-01, -3.0592e-01,  1.5885e+00, -2.1776e-01,\n",
            "          4.8043e-01, -8.9693e-01,  6.2812e-01, -8.0487e-01,  2.4533e-01,\n",
            "         -2.2773e-01, -4.9023e-01, -5.7397e-02, -7.1978e-02,  6.8417e-01,\n",
            "          3.0468e-01, -2.2018e-01,  1.7091e-01, -4.3361e-01, -1.3614e-01,\n",
            "         -5.2088e-01,  6.4118e-01, -5.3125e-01,  1.4665e+00,  1.5174e+00,\n",
            "          9.7967e-01,  5.0859e-01, -6.6972e-01,  1.9123e-01,  5.3745e-01,\n",
            "         -1.2128e-01,  7.6711e-02, -3.0778e-01, -4.6009e-02,  8.0425e-01,\n",
            "         -3.7026e-01,  1.5258e-01, -1.3606e+00, -3.5183e-01,  5.1647e-01,\n",
            "          1.1943e-01, -4.1414e-01, -1.8972e-01, -7.1182e-02,  4.3164e-01,\n",
            "         -1.3351e+00, -6.8693e-01,  1.0996e+00,  6.7960e-02, -9.8906e-02,\n",
            "          6.6148e-01,  1.6139e-01,  1.0055e-01, -4.1256e-01,  2.3737e-01,\n",
            "          1.1871e+00, -6.2013e-01, -1.7179e-01,  8.6545e-01, -1.9558e+00,\n",
            "         -4.5724e-01,  7.0063e-02, -1.1102e+00, -1.0603e+00, -7.0850e-01,\n",
            "         -3.2149e-01,  6.1530e-02,  2.6978e-01,  1.0194e+00,  6.3125e-01,\n",
            "         -5.0130e-01,  7.3629e-01,  8.3672e-01,  6.4149e-01, -1.3437e+00,\n",
            "         -3.4268e-01, -7.7218e-01,  7.9189e-01,  5.3550e-01,  6.0389e-02,\n",
            "         -4.3916e-01,  8.1574e-01,  3.5993e-01,  6.3937e-01, -1.0572e-01,\n",
            "         -1.0363e-01,  9.1991e-01,  4.5471e-01, -5.0576e-01,  9.2609e-01,\n",
            "         -1.6616e-01, -7.8292e-01, -3.1712e-01, -7.2174e-01,  9.4854e-02,\n",
            "          1.2569e+00,  8.3142e-01,  3.1027e-01, -5.5438e-01,  1.6920e-01,\n",
            "          1.3815e+00, -4.0292e-01, -5.4076e-01, -1.1493e-01,  3.8152e-02,\n",
            "          1.0589e+00,  1.4502e-01, -2.2963e-01,  6.4277e-01,  2.8132e-01,\n",
            "          7.9862e-01, -1.3653e-01, -9.6251e-01,  6.8168e-03,  2.7801e-02,\n",
            "         -1.1833e+00,  1.4858e-01,  1.0511e+00, -4.3835e-01, -2.3269e-01,\n",
            "          2.9281e-01, -3.9291e-01,  1.0341e+00, -3.5507e-01,  4.7759e-02,\n",
            "         -2.9888e-01, -5.4615e-01,  1.1440e+00, -4.4812e-01,  3.6858e-02,\n",
            "         -3.2022e-02, -4.7930e-01, -6.7097e-01,  1.1335e+00,  9.6531e-01,\n",
            "         -6.6374e-01, -7.5585e-01,  3.6964e-01, -6.2033e-01,  1.1047e+00,\n",
            "          9.3066e-02,  5.0421e-01, -4.1701e-01, -9.0490e-01, -8.4097e-01,\n",
            "          1.1018e-01,  7.5689e-01,  1.7275e-01, -7.6223e-01,  5.7520e-01,\n",
            "          4.2371e-02,  4.7680e-01, -7.0713e-01, -3.8828e-02,  4.2641e-01,\n",
            "          3.5636e-01,  1.1014e-01, -3.6608e-02,  3.1858e-01, -3.8282e-01,\n",
            "         -9.9895e-01,  1.2076e-01, -1.3374e+00,  7.0342e-01,  7.1355e-01,\n",
            "          1.2920e+00, -1.7174e-01, -4.9302e-02, -9.7964e-01, -1.2666e-02,\n",
            "          5.7145e-02,  1.9793e+00,  1.0565e-01, -4.7164e-01,  6.1177e-01,\n",
            "         -6.7420e-01,  2.4594e-02,  4.0145e-01,  5.0064e-01,  5.3819e-01,\n",
            "         -7.8765e-02, -2.2561e-01, -1.0589e+00, -6.7161e-01, -1.3370e+00,\n",
            "         -2.8852e-01, -1.0146e-01,  6.1145e-01, -8.7811e-01, -6.9947e-01,\n",
            "         -6.3487e-01, -5.9324e-01,  1.2261e+00, -1.5011e+00,  1.1196e+00,\n",
            "          2.3160e-01, -5.6532e-01,  3.3098e-01,  5.3730e-02,  9.3415e-01,\n",
            "         -1.6406e-01,  9.1403e-03,  4.8434e-02,  6.8331e-02,  9.4644e-01,\n",
            "          2.7792e-01,  3.1853e-01, -6.3992e-01,  6.2178e-01,  2.1019e-01,\n",
            "          2.3101e-01,  1.0277e+00,  1.2515e-02,  4.2150e-01, -7.2762e-01,\n",
            "          5.8563e-02, -4.5661e-01,  3.2690e-01,  3.1199e-01,  1.0504e-01,\n",
            "         -7.6822e-01, -1.0306e+00, -1.9351e-01, -4.2837e-01, -5.1585e-01,\n",
            "         -8.6734e-02, -2.5916e-01, -4.4342e-02,  3.0075e-01,  5.6710e-01,\n",
            "          1.4316e+00, -1.0461e+00,  4.0321e-01, -4.2482e-01,  3.9069e-01,\n",
            "         -5.3215e-02, -7.6812e-02, -8.1557e-01, -6.1453e-01, -3.0923e-01,\n",
            "          4.3192e-01, -5.1136e-01, -8.7456e-01,  1.9333e-01, -7.0170e-01,\n",
            "          1.7531e+00,  7.1061e-01, -5.3399e-01, -1.1258e-02, -1.4031e-01,\n",
            "         -4.0806e-01,  1.7729e-01, -7.3940e-01, -4.3006e-01,  1.4778e-01,\n",
            "         -7.6466e-01,  1.1463e+00,  2.9309e-01,  1.3241e-01,  6.7125e-01,\n",
            "         -1.8531e-01,  4.2409e-01,  9.2425e-02, -2.2919e-02,  3.3750e-02,\n",
            "         -6.0580e-01, -6.2436e-01,  4.3790e-01,  3.6833e-01,  6.2528e-01,\n",
            "         -1.4303e-01, -2.6318e-01, -3.8953e-01,  1.5864e-01, -1.3642e-01,\n",
            "         -5.9182e-01, -3.3539e-01,  1.3203e+00, -5.0096e-01,  1.5245e+00,\n",
            "          1.0805e-01, -1.2631e-02, -1.8749e-01, -5.2178e-01, -3.5947e-02,\n",
            "         -3.1489e-01,  9.6305e-02,  3.8667e-01, -3.5292e-01, -5.0717e-02,\n",
            "         -1.1000e-01,  7.6344e-01, -1.3826e-02, -2.7162e-01, -1.1048e-01,\n",
            "         -8.1743e-01, -9.0955e-01, -4.0890e-01, -1.3077e+00, -6.8516e-01,\n",
            "          3.4415e-01,  6.3110e-01,  3.9104e-02,  1.1239e+00,  3.4018e-01,\n",
            "          4.5994e-01, -2.9974e-01,  5.7424e-01, -1.2917e+00, -2.7770e-01,\n",
            "         -1.4407e+00,  4.9791e-01,  8.6339e-01, -2.2258e-02,  2.3699e-01,\n",
            "          7.2081e-02,  1.2197e-02,  1.7812e-01,  2.8017e-01, -2.0013e-01,\n",
            "         -1.0811e-01,  8.3850e-01, -5.0951e-02, -2.1956e-01, -3.4104e-01,\n",
            "          2.8042e-01,  7.4830e-01, -3.0467e-01, -2.1437e-02,  2.4523e-01,\n",
            "          1.9107e+00, -7.2158e-01, -1.7347e-01,  2.9271e-02,  6.6414e-01,\n",
            "         -3.7068e-01,  3.4662e-01, -1.3598e-01, -2.3390e-01,  7.1911e-02,\n",
            "         -3.4857e-01,  3.2390e-01,  4.5053e-01,  2.7539e-01,  4.2396e-01,\n",
            "         -3.3908e-01,  1.2414e-01, -4.7158e-02,  4.2349e-01,  2.0388e-01,\n",
            "         -2.8991e-01, -6.0664e-01, -1.0557e+00,  6.8169e-01,  1.4372e-01,\n",
            "          4.9542e-01,  5.2802e-02, -4.3358e-01, -1.0154e+00,  7.0501e-01,\n",
            "         -7.1561e-01,  1.4681e+00,  1.7612e-01,  7.3543e-01,  3.1453e-01,\n",
            "         -1.7526e-01, -2.6634e-01,  1.2080e+00, -3.5939e-01, -1.0774e+00,\n",
            "         -5.3320e-01, -9.6798e-01, -2.1823e-01, -2.2483e-02, -1.6394e+00,\n",
            "          5.4933e-02, -1.5625e-01, -7.1890e-01, -5.1505e-01,  9.7430e-02,\n",
            "         -5.8235e-01, -3.7881e-01,  1.5534e-01,  2.9725e-01, -4.5980e-01,\n",
            "         -1.4106e+00,  3.5009e-01,  2.9507e-01,  8.0823e-01,  1.4402e+00,\n",
            "         -4.7553e-01,  5.9858e-01, -6.6094e-01,  7.9237e-01, -1.5184e+00,\n",
            "         -4.7851e-01, -5.7514e-01,  1.7030e+00, -4.1418e-01,  4.4532e-01,\n",
            "          9.1360e-01, -2.6257e-01,  1.0812e+00,  3.8779e-01,  1.0327e+00,\n",
            "          7.5560e-01,  7.1686e-01, -3.6724e-01, -9.5899e-01,  1.9424e-01,\n",
            "         -1.8445e-01,  3.9652e-01, -9.1429e-04, -1.4303e+00, -3.2508e-01,\n",
            "         -1.0903e+00],\n",
            "        [ 2.5603e-01, -7.1324e-01, -7.1967e-01, -8.4554e-01, -6.4591e-01,\n",
            "          1.3197e+00,  7.6177e-01,  2.6632e-01,  4.3446e-01, -7.9389e-01,\n",
            "          7.2788e-01,  3.5445e-01, -3.7735e-01,  3.3504e-01,  9.3321e-01,\n",
            "         -2.2945e-01,  1.6167e-01,  1.3383e-02, -9.1503e-01, -1.1522e-01,\n",
            "         -6.1435e-01,  1.8199e+00,  6.6777e-01,  1.1592e+00, -1.0737e-01,\n",
            "          5.4494e-01, -1.2062e+00,  1.5319e+00, -1.4490e-01, -7.2997e-01,\n",
            "         -1.5604e+00, -5.0435e-01,  3.2512e-01,  6.5534e-02, -9.1867e-01,\n",
            "          2.7986e-01, -1.5058e+00,  7.3777e-01, -1.3063e+00, -1.1860e+00,\n",
            "         -1.3438e-01,  1.3672e-03,  8.8443e-01,  7.1328e-02, -6.4961e-01,\n",
            "          1.1508e+00, -3.7876e-01, -1.0950e+00, -1.4732e+00,  9.0619e-01,\n",
            "         -1.2413e+00, -5.0067e-01, -5.2251e-01,  4.3243e-01,  1.1773e+00,\n",
            "          3.7435e-01, -5.6747e-01, -5.6075e-01, -4.4487e-01, -9.4211e-01,\n",
            "          1.3886e+00,  6.1655e-01, -2.9219e-02, -6.9411e-01,  1.5772e+00,\n",
            "         -2.4705e-01, -2.0996e+00, -9.5644e-01,  7.8392e-01,  3.7212e-01,\n",
            "          1.1443e+00, -9.1701e-02,  3.0563e-01, -1.1534e+00,  3.0504e-01,\n",
            "         -3.3520e-01,  1.2870e+00, -3.9834e-01, -1.0124e+00,  7.4552e-01,\n",
            "          7.0079e-01, -1.3062e-01,  2.6059e-01, -1.1108e+00, -2.4825e+00,\n",
            "         -6.9267e-01, -8.9930e-01, -7.5877e-01, -8.0919e-01,  3.0447e-01,\n",
            "         -3.3349e-01,  2.6946e-01,  5.7015e-01,  8.6994e-01, -7.2145e-01,\n",
            "          8.7883e-01, -1.4392e+00,  1.5426e-01, -7.9254e-01,  1.9998e-01,\n",
            "          8.5024e-01, -2.6332e-01,  1.2069e+00,  1.3135e-01, -8.3295e-02,\n",
            "         -1.6019e+00,  1.0831e+00, -4.1808e-01,  1.3611e+00, -5.6642e-01,\n",
            "         -1.1248e+00, -2.6765e-01,  1.2137e+00, -8.9705e-01, -6.1245e-01,\n",
            "         -1.3998e+00, -8.7452e-01, -1.2425e+00,  1.2775e+00,  1.0462e+00,\n",
            "          2.2433e-02, -1.3156e-01,  9.4560e-02,  5.2234e-01, -1.1868e+00,\n",
            "         -2.1071e-01,  7.9099e-01, -9.5959e-01, -5.5506e-01,  4.7540e-01,\n",
            "         -5.6722e-01,  4.8215e-01, -3.9175e-01, -2.5481e-01,  5.2872e-01,\n",
            "         -6.2592e-02,  9.0851e-01, -1.1996e+00,  4.4784e-01, -3.8398e-01,\n",
            "          7.3516e-02,  6.9608e-01, -6.5399e-01,  9.1876e-01,  1.0456e+00,\n",
            "         -4.1964e-01,  3.6172e-01,  9.2633e-01, -2.7194e-01, -3.4964e-01,\n",
            "         -8.7674e-01,  6.4280e-01,  2.9639e-01, -1.2666e+00,  5.2152e-01,\n",
            "          7.8229e-01, -8.6223e-01,  2.1443e+00,  1.8443e+00,  6.5370e-01,\n",
            "          1.4375e-01, -1.1078e+00,  2.5774e-01,  1.4006e+00, -6.0557e-02,\n",
            "         -6.0645e-01, -5.6105e-01, -4.3706e-02, -6.1488e-02,  2.2302e-01,\n",
            "         -8.1128e-01,  8.4881e-01,  9.8719e-01,  6.6740e-02, -4.3942e-01,\n",
            "          6.0916e-01, -1.0401e+00, -1.6904e-01, -7.9668e-01,  2.8153e-01,\n",
            "          1.3239e+00,  1.0178e+00, -5.1435e-01,  1.4125e+00,  2.9240e-01,\n",
            "          4.4670e-01, -5.4484e-01,  4.2875e-01, -3.3974e-01,  3.8248e-01,\n",
            "          4.9644e-01, -1.0042e+00, -1.7849e+00, -3.7511e-01,  1.5846e+00,\n",
            "         -3.7804e-03,  4.9275e-02,  4.0950e-01,  1.0768e+00, -7.0005e-01,\n",
            "          1.4701e+00,  1.7109e-01, -1.9529e-01, -3.4588e-01,  3.8041e-01,\n",
            "         -3.3773e-01, -1.1314e+00, -2.4053e+00,  6.6326e-01, -8.9633e-01,\n",
            "          1.2384e+00,  1.5494e-01, -5.1893e-01,  1.8940e-02,  7.7267e-01,\n",
            "          1.0871e-01,  5.6111e-01,  1.2630e+00, -7.6388e-01, -5.8697e-03,\n",
            "         -3.6886e-02, -1.3012e+00, -6.2903e-01, -9.4983e-01,  1.2121e+00,\n",
            "         -7.7395e-01, -7.9955e-01, -2.4683e-02, -6.2256e-01, -1.2200e+00,\n",
            "         -3.2801e-01, -5.5645e-01,  5.4229e-01, -2.9409e-01, -7.3464e-01,\n",
            "          1.9392e-01, -1.0395e-01, -1.1647e+00, -9.9294e-03, -2.8010e-02,\n",
            "         -8.5605e-01,  6.6953e-01,  5.9072e-01,  2.7137e-01,  1.7121e-01,\n",
            "         -9.0983e-01,  3.6251e-01, -2.6866e-01, -6.6471e-01,  6.2880e-01,\n",
            "         -4.2859e-01,  3.2046e-02,  2.2932e-01,  5.2863e-01,  2.1417e-01,\n",
            "         -2.8897e-01,  2.4733e-01,  4.8851e-02, -6.4772e-01, -1.0839e+00,\n",
            "         -5.1585e-01,  6.7840e-01, -5.3798e-01, -6.7101e-01,  1.2739e+00,\n",
            "          6.9670e-02,  1.0125e-02, -5.2771e-01, -9.6526e-01,  1.2259e-01,\n",
            "          4.3712e-01, -5.5094e-01, -1.6795e+00, -1.5128e+00, -4.8495e-01,\n",
            "         -1.5389e+00, -9.9888e-01, -9.1453e-02,  1.5259e+00,  1.1483e+00,\n",
            "          1.1097e+00,  9.5077e-01,  6.7717e-01, -5.1033e-01, -1.2239e+00,\n",
            "          9.2210e-01, -1.4161e+00,  1.4811e+00, -1.7611e-01,  8.9202e-01,\n",
            "          4.7485e-01, -5.2845e-01,  1.2280e-01,  1.7376e+00,  8.1058e-01,\n",
            "         -9.6744e-01, -5.3657e-01,  1.0842e+00, -7.4364e-01,  2.8135e-01,\n",
            "          4.3070e-01,  1.6902e+00, -2.6781e-01,  2.3456e-01, -1.4908e+00,\n",
            "         -1.6727e+00,  9.7076e-03,  1.8494e-01, -9.2255e-01, -1.0548e+00,\n",
            "          7.3956e-01, -7.6959e-02, -1.1083e+00, -3.0063e-01,  3.5735e-01,\n",
            "         -7.4811e-02,  1.1821e+00, -3.0897e-01, -1.2252e-01,  8.8708e-01,\n",
            "         -1.3590e+00, -6.8218e-01,  1.0654e-01, -1.8007e-01,  1.5150e+00,\n",
            "         -4.1820e-01, -5.5596e-01,  1.2887e+00, -4.4786e-01,  9.3518e-01,\n",
            "          7.8680e-01,  5.3313e-01, -1.5024e-01,  1.2865e+00,  1.0876e+00,\n",
            "          1.3063e-01, -3.0401e-02,  1.0846e+00,  1.1851e+00, -4.2530e-01,\n",
            "          8.8967e-01,  1.1379e-01,  7.7512e-01,  8.1924e-01, -6.4804e-01,\n",
            "         -3.2721e-01, -3.8847e-01,  1.1013e+00, -6.1011e-01,  6.8301e-01,\n",
            "         -5.6665e-01,  2.6738e-01, -9.1218e-01,  5.0870e-01, -5.8891e-01,\n",
            "          1.1564e+00,  3.8836e-01, -5.8187e-01,  2.7406e-01, -1.8718e-01,\n",
            "         -1.0160e+00, -2.8389e-01, -1.9977e-01, -2.5817e-01,  1.6194e-01,\n",
            "          4.4441e-01, -4.8075e-01, -3.5324e-01,  2.3039e-01,  2.8011e-02,\n",
            "          5.2083e-01, -5.3109e-01, -1.6585e+00, -5.4438e-01,  4.3566e-01,\n",
            "         -1.2875e-01, -2.0025e-01, -2.7136e-01, -1.1682e-02, -1.7767e+00,\n",
            "          6.8691e-01, -4.9904e-01, -1.4772e+00,  1.6936e+00, -1.1119e+00,\n",
            "         -3.3611e-01,  1.6016e-01, -2.4432e-01, -3.9504e-01,  8.9875e-01,\n",
            "          2.9992e-01,  1.1382e+00,  4.4606e-01,  2.7185e+00, -1.0644e+00,\n",
            "          1.0307e+00, -3.1823e-01,  9.0443e-01, -3.1055e-01, -1.2264e+00,\n",
            "         -4.9938e-01,  1.0359e+00,  1.4753e+00, -2.6225e-01, -1.3580e+00,\n",
            "          1.0898e+00, -5.0837e-02,  5.2203e-02, -9.9044e-01,  6.7885e-01,\n",
            "          1.0061e+00, -7.1469e-01,  1.2832e+00,  2.1572e-01, -4.3626e-01,\n",
            "         -1.0389e+00, -5.3543e-01,  5.3068e-01, -1.7765e+00, -2.7684e-01,\n",
            "          2.9175e-01, -1.6760e-01,  7.0996e-01,  1.8965e+00, -1.0857e-01,\n",
            "          7.1688e-01,  9.2957e-01, -5.1933e-01,  4.6385e-01,  1.3670e+00,\n",
            "         -3.4656e-01]], grad_fn=<SliceBackward0>) \n",
            " tensor([137, 361, 200, 282,  70, 373, 396, 309,  59,  31,  54, 407, 220,  76,\n",
            "         77, 135, 198, 322, 412, 226, 260, 127,  26, 313, 158, 152, 404, 420,\n",
            "          4,  18, 184, 193])\n"
          ]
        }
      ],
      "source": [
        "print(outputs[:2], \"\\n\", labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prqXwKQx5DkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efde078c-f5a1-4479-89bf-7ce47ff557a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.370771408081055\n"
          ]
        }
      ],
      "source": [
        "loss_fn = F.cross_entropy\n",
        "loss = loss_fn(outputs, labels)\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seGtQLql5Ouo"
      },
      "source": [
        "**Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-3kXf7O5NxK"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnzI_HLK5Xj-"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Now that we have defined the data loaders, model, loss function and optimizer, we are ready to train the model. The training process is identical to linear regression, with the addition of a \"validation phase\" to evaluate the model in each epoch. Here's what it looks like in pseudocode:\n",
        "\n",
        "```\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    for batch in train_loader:\n",
        "        # Generate predictions\n",
        "        # Calculate loss\n",
        "        # Compute gradients\n",
        "        # Update weights\n",
        "        # Reset gradients\n",
        "    \n",
        "    # Validation phase\n",
        "    for batch in val_loader:\n",
        "        # Generate predictions\n",
        "        # Calculate loss\n",
        "        # Calculate metrics (accuracy etc.)\n",
        "    # Calculate average validation loss & metrics\n",
        "    \n",
        "    # Log epoch, loss & metrics for inspection\n",
        "```\n",
        "\n",
        "Some parts of the training loop are specific the specific problem we're solving (e.g. loss function, metrics etc.) whereas others are generic and can be applied to any deep learning problem. \n",
        "\n",
        "We'll include the problem-independent parts within a function called `fit`, which will be used to train the model. The problem-specific parts will be implemented by adding new methods to the `nn.Module` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhbJft2t4BUM"
      },
      "outputs": [],
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None):\n",
        "    # Calculate Loss\n",
        "    preds = model(xb)\n",
        "    loss = loss_func(preds, yb)\n",
        "    \n",
        "    if opt is not None:\n",
        "        # Compute Gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        opt.step()\n",
        "        # Reset gradients\n",
        "        opt.zero_grad()\n",
        "    \n",
        "    metric_result = None\n",
        "    if metric is not None:\n",
        "        # Compute the metric\n",
        "        metric_result = metric(preds, yb)\n",
        "    return loss.item(), len(xb), metric_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH89YBZv5e4c"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
        "    with torch.no_grad():    # Gradient Descent is not needed for validation\n",
        "        # Pass each batch through the model\n",
        "        results = [loss_batch(model, loss_fn, xb, yb, metric=metric) for xb, yb in valid_dl]\n",
        "        # Separate losses, counts and metrics\n",
        "        losses, batch_num, metrics = zip(*results)\n",
        "        # Total size of dataset\n",
        "        total_batch = np.sum(batch_num)\n",
        "        # Average loss across batches\n",
        "        avg_loss = np.sum(np.multiply(losses, batch_num)) / total_batch\n",
        "        avg_metric = None\n",
        "        if metric is not None:\n",
        "            # Avg of metric across batches\n",
        "            avg_metric = np.sum(np.multiply(metrics, batch_num)) / total_batch\n",
        "    return avg_loss, total_batch, avg_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBJPmkRd5hYm"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    max_value, pred_idxs = torch.max(outputs, dim=1)\n",
        "    return torch.sum(pred_idxs == labels).item() / len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRl8fOAS4GDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cae243b-359e-4423-da69-f103343938c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 6.3310, Accuracy: 0.0021\n"
          ]
        }
      ],
      "source": [
        "val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric=accuracy)\n",
        "print(\"Loss: {:.4f}, Accuracy: {:.4f}\".format(val_loss, val_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-PhZd9k77PN"
      },
      "source": [
        "***We can now define the `fit()` function using `loss_batch()` and `evaluate()`.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqZId6VI780R"
      },
      "outputs": [],
      "source": [
        "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
        "    accuracies = []\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        for xb, yb in train_dl:\n",
        "            loss, _, _ = loss_batch(model, loss_fn, xb, yb, opt)\n",
        "        # Evaluation\n",
        "        val_loss, total_batches, val_acc = evaluate(model, loss_fn, valid_dl, metric)\n",
        "        accuracies.append(val_acc)\n",
        "        # Printing the progresses\n",
        "        if metric is None:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, val_loss))\n",
        "        else:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
        "                          .format(epoch+1, epochs, val_loss, metric.__name__, val_acc))\n",
        "            \n",
        "    return accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paHm32gz8E9i"
      },
      "outputs": [],
      "source": [
        "# Redefining the model and optimizer\n",
        "model = CarModel()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "loss_fn = F.cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nti45Xn78MgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21507a3a-6c16-47fa-ff12-264fe7f341c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 8.2873, accuracy: 0.0172\n",
            "Epoch [2/5], Loss: 9.6327, accuracy: 0.0140\n",
            "Epoch [3/5], Loss: 8.0624, accuracy: 0.0293\n",
            "Epoch [4/5], Loss: 8.3666, accuracy: 0.0256\n",
            "Epoch [5/5], Loss: 9.1751, accuracy: 0.0251\n",
            "['0.0172', '0.0140', '0.0293', '0.0256', '0.0251']\n"
          ]
        }
      ],
      "source": [
        "accuracies = fit(5, model, loss_fn, optimizer, train_loader, val_loader, accuracy)\n",
        "print([\"{:.4f}\".format(accur) for accur in accuracies])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38Cbre61CrDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c871f693-f3e9-47df-faf8-8c17ec3239ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0172\n",
            "Accuracy: 0.0140\n",
            "Accuracy: 0.0293\n",
            "Accuracy: 0.0256\n",
            "Accuracy: 0.0251\n"
          ]
        }
      ],
      "source": [
        "for accuracy in accuracies:\n",
        "    print(\"Accuracy: {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KOzykqNmLFEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the accuracy does continue to increase as we train for more epochs, the improvements get smaller with every epoch. Let's visualize this using a line graph."
      ],
      "metadata": {
        "id": "XmyaLQtiWs5J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4od3RVBC4QR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f10a5d7f-cbaf-4ea7-b1e1-ade2a9d4fbd0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(accuracies, '--*')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(\"Accuracy vs no. of Epochs\");"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "LogisticRegression_Car_Prediction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYVrIvYjMsFJU8H6VW/JGL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}